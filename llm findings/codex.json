{
  "source": "Codex",
  "findings": [
    {
      "id": "F-001",
      "title": "Header protection outputs are ignored; packet-number length + key phase are derived from protected header",
      "category": "security",
      "severity": "S0",
      "confidence": "high",
      "impact": "- The RX path will decode packet numbers using the wrong length and the wrong key phase for a large fraction of packets.\n- This is not just \u201chandshake sometimes fails\u201d; it can become memory-unsafe because `pkt_num_len` controls reads from `data + ctx.offset` and advances `ctx.offset`.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [
          "and",
          "bits",
          "tquic_hp_unprotect",
          "tquic_remove_header_protection"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2528-2528",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2539-2539",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:302-302"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Treat `tquic_hp_unprotect()` as authoritative for `pn_len` and (short header) `key_phase`.\n- After HP removal, recompute any fields derived from the masked bits (pn length, key phase, reserved bits validation).\n- In short header parsing: do not interpret spin/key-phase/pn-len prior to unprotect.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-002",
      "title": "Packet number reconstruction always uses `largest_pn = 0`",
      "category": "correctness",
      "severity": "S0",
      "confidence": "high",
      "impact": "- Truncated packet numbers cannot be reconstructed correctly past the very beginning of a connection.\n- This will cause persistent decryption failures and protocol failure even if HP/pn_len are fixed.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [
          "space",
          "tquic_decode_pkt_num"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2572-2572"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Track `largest_pn` per PN space (Initial/Handshake/Application) and pass it into `tquic_decode_pkt_num()`.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-003",
      "title": "CID demux/lookup appears non-functional: the RX path uses one table, while connection creation populates different tables",
      "category": "security",
      "severity": "S0",
      "confidence": "high",
      "impact": "- Incoming packets cannot reliably find an existing connection by DCID because the table used by lookup is not obviously populated by the connection-creation path.\n- This is a fundamental functional defect (protocol will appear \u201cdead\u201d), and it makes any further bug-hunting noisy because everything collapses into \u201cno connection found.\u201d",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/connection.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_connection.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_cid.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [
          "defect",
          "tables",
          "tquic_cid_hash_add",
          "tquic_cid_issue",
          "tquic_cid_lookup",
          "tquic_cid_pool_init",
          "tquic_conn_lookup_by_cid",
          "tquic_lookup_by_dcid"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/connection.c:2696-2696",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_connection.c:463-463",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_cid.c:530-530",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:222-222"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Pick exactly one authoritative CID demux table for RX and ensure connection creation inserts the SCID/DCIDs into it.\n- Delete or hard-disable the unused tables (or clearly fence them behind config/ifdef) to reduce attack surface and confusion.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-004",
      "title": "rhashtable/RCU lifetime issues (use-after-free risk) in CID tables",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "- If any of these tables are used from softirq (packet RX) concurrently with teardown/rotation, the current code can dereference freed objects.\n- Kernel UAF is a high severity security bug class.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_connection.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_cid.c"
        ],
        "symbols": [
          "call_rcu",
          "issues",
          "kfree_rcu",
          "rcu_read_lock",
          "rhashtable_lookup_fast",
          "softirq",
          "synchronize_rcu"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_cid.c:355-355",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_cid.c:537-537"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Decide on a correct concurrency model:\n  - Option A: Use rhashtable in the intended RCU mode.\n    - Lookups under `rcu_read_lock()`.\n    - Deletions free via `kfree_rcu()` / `call_rcu()` (or `synchronize_rcu()` prior to free).\n    - Consider `SLAB_TYPESAFE_BY_RCU` for caches holding RCU-freed objects.\n  - Option B: Do not use lockless lookups.\n    - Protect *all* lookup/insert/remove with a lock and use the non-fast lookup variant as appropriate.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-005",
      "title": "RX parsing/decryption assumes contiguous skb data (non-linear skb / GRO risk)",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "- If skb payload is not fully linear, decryption and frame parsing can read beyond the linear head area, corrupt memory, or crash.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [
          "data",
          "skb_copy_bits",
          "skb_linearize",
          "sockets"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Linearize the skb before any parsing/decryption that assumes contiguity, or use `skb_copy_bits()` into a contiguous buffer for header+payload.\n- Do this *before* using `data[offset]` patterns.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-006",
      "title": "QUIC-Exfil mitigation code uses `skb->cb` as a function-pointer slot and gates on `skb->cb[0]`",
      "category": "concurrency",
      "severity": "S0",
      "confidence": "high",
      "impact": "- Functional: jitter/timing queues will non-deterministically drop packets depending on callback pointer low byte.\n- Safety: `skb->cb` is a shared control block used by networking layers; repurposing it for function pointers is extremely brittle and can become exploitable if any path ever queues an skb with unexpected `cb` contents.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c"
        ],
        "symbols": [
          "object",
          "void"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c:1090-1090",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c:270-270"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Never store function pointers in `skb->cb`.\n- Use a wrapper object (`struct { struct sk_buff *skb; void (*send_fn)(...); }`) in a dedicated queue, or at least define a strict `struct` overlay for `skb->cb` and always set a sentinel + zero the rest.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-007",
      "title": "Widespread allocator mismatches (kmem_cache vs kzalloc/kfree) for core objects (conn/path/stream)",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "- These are not \u201cmaybe\u201d issues; allocator mismatches in kernel space are a direct path to slab corruption, UAF, and hard-to-debug crashes.\n- Because these objects are reachable from network ingress and from netlink/diagnostic paths, this is also a security boundary issue, not just functional breakage.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/Makefile",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_connection.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_path.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_main.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_migration.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_stream.c"
        ],
        "symbols": [
          "Evidence",
          "kfree",
          "kmem_cache_free",
          "mismatches",
          "objects",
          "tquic_conn_alloc",
          "tquic_conn_destroy",
          "tquic_conn_free",
          "tquic_path_alloc",
          "tquic_path_create",
          "tquic_path_destroy",
          "tquic_path_free",
          "tquic_stream_alloc",
          "tquic_stream_free",
          "type"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/Makefile:18-18",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_connection.c:428-428",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_connection.c:965-965",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/quic_path.c:323-323",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_main.c:157-157",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_main.c:182-182",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_main.c:212-212",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_migration.c:403-403",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_stream.c:299-299"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Pick exactly one allocator strategy per object type (`tquic_connection`, `tquic_path`, `tquic_stream`) and enforce it via dedicated wrappers:\n  - `tquic_conn_alloc()/tquic_conn_free()`\n  - `tquic_path_alloc()/tquic_path_free()`\n  - `tquic_stream_alloc()/tquic_stream_free()`\n- Remove or fence any alternate implementation files that allocate the same types differently, or clearly split the types so the compiler can\u2019t mix them.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-008",
      "title": "Reference counting/RCU lifetime is not actually enforced; direct `tquic_conn_destroy()` calls can free live connections",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "- Any subsystem that holds a refcounted pointer (netlink path manager, diagnostics, timer/work subsystems) can observe freed memory if another path calls `tquic_conn_destroy()` directly.\n- This is a classic high-severity kernel UAF class bug.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/include/net/tquic.h",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/pm/path_manager.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_handshake.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_socket.c"
        ],
        "symbols": [
          "assertions",
          "internal",
          "pointer",
          "rcu_read_lock",
          "refcount_inc_not_zero",
          "sites",
          "tquic_conn_destroy",
          "tquic_conn_get",
          "tquic_conn_lookup_by_token",
          "tquic_conn_put"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/include/net/tquic.h:1965-1965",
          "/Users/justinadams/Downloads/tquic-kernel/include/net/tquic.h:1981-1981",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/pm/path_manager.c:1136-1136",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_socket.c:171-171"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Make `tquic_conn_destroy()` private/internal (not a general-purpose public API). Enforce that all external callers use `tquic_conn_put()` and that the destructor only runs on the refcount-to-zero edge.\n- Add debug assertions (at least under `CONFIG_DEBUG_KERNEL`) to catch direct destruction with `refcnt != 1` or while still discoverable in lookup tables/lists.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-009",
      "title": "Global connection hashtable (`tquic_conn_table`) is initialized and removed-from, but never inserted-into",
      "category": "correctness",
      "severity": "S0",
      "confidence": "high",
      "impact": "- Debug/proc/diag and netns cleanup code that iterates `tquic_conn_table` will not see live connections.\n- `rhashtable_remove_fast()` on an element that was never inserted is at best a silent logic bug and at worst a correctness hazard depending on rhashtable expectations.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_main.c"
        ],
        "symbols": [
          "hashtable",
          "rhashtable_remove_fast",
          "tquic_conn_lookup_by_token"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_main.c:145-145",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_main.c:796-796"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Either wire up insertion consistently at connection establishment, or delete `tquic_conn_table` and all iteration users in favor of the per-netns list already used by `tquic_conn_lookup_by_token()`.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-010",
      "title": "`sockaddr_storage` path lookup uses `memcmp()` over the full struct (padding-sensitive)",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "- A path can fail to be found if two logically equal addresses differ in padding bytes or unused fields. This breaks multipath/path binding and can cascade into bad routing decisions.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [
          "memcmp",
          "struct"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:245-245"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Compare by `ss_family`, then compare only the relevant address+port fields for that family.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-011",
      "title": "`setsockopt(SOL_TQUIC, ...)` forces `optlen >= sizeof(int)` even for string/binary options",
      "category": "api",
      "severity": "S1",
      "confidence": "medium",
      "impact": "- API incompatibility and hard-to-use userspace interface (e.g., small strings rejected).\n- Unnecessary userspace reads on every setsockopt call.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_socket.c"
        ],
        "symbols": [
          "copy_from_sockptr",
          "interface",
          "setsockopt",
          "tquic_sock_setsockopt"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Move `copy_from_sockptr(&val, ...)` inside the integer-valued cases only.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-012",
      "title": "Unit tests model packet-number length as readable from the first byte without HP removal",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "- This test model will not catch the P0 HP/pn_len bug; it encodes the same incorrect assumption.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/test/packet_test.c"
        ],
        "symbols": [
          "Plan",
          "concurrency",
          "skbs",
          "tquic_process_packet",
          "tquic_udp_recv"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "- Add tests that build a packet with header protection applied, run HP unprotect, and then validate pn_len and key_phase.",
      "tests_to_add": [
        "`/Users/justinadams/Downloads/tquic-kernel/net/tquic/test/packet_test.c` asserts `(header & 0x03) + 1` as \u201cpacket number length\u201d directly.",
        "This test model will not catch the P0 HP/pn_len bug; it encodes the same incorrect assumption.",
        "Add tests that build a packet with header protection applied, run HP unprotect, and then validate pn_len and key_phase."
      ],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-013",
      "title": "Stream Frame Without Length Allows Reading Past Decrypted Buffer",
      "category": "correctness",
      "severity": "S0",
      "confidence": "high",
      "impact": "** LOW - the design is correct as long as the frame dispatcher respects the protocol requirement that a frame without a length field must be the last frame. However, the code lacks an explicit assertion or comment in the dispatcher to enforce this invariant.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [
          "dispatcher"
        ],
        "line_ranges": [],
        "snippets": [
          "} else {\n    /* Length extends to end of packet */\n    length = ctx->len - ctx->offset;\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** When a STREAM frame does not have the LENGTH bit set, the code assumes the data extends to the end of the buffer. However, `ctx->len` represents the total payload length, which may include subsequent frames in the same packet. Per RFC 9000 Section 19.8, a STREAM frame without the LENGTH bit \"extends to the end of the packet.\" Since this is the last frame in a packet, this is correct in theory, but the frame dispatcher (the caller of this function) should not call any further frame processing after a length-less STREAM frame. If the dispatcher continues processing, it would find `ctx->offset == ctx->len` and cleanly exit, so this is safe.",
      "fix_suggestion": "** Add a flag to `tquic_rx_ctx` that records when a length-less STREAM frame is processed, and assert in the dispatcher that no further frames follow.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-014",
      "title": "STREAM Frame `length` Validation Allows Up to 65535 Bytes Per Frame",
      "category": "correctness",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** MEDIUM - could cause interoperability issues with peers sending large STREAM frames. The limit should be the remaining packet length, which is already checked on line 912.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [
          "overhead"
        ],
        "line_ranges": [],
        "snippets": [
          "if (length > 65535)\n    return -EINVAL;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** This limit is arbitrary and not from the RFC. RFC 9000 does not impose a per-frame length limit beyond the packet size. While the intent is resource protection, the limit means legitimate peers sending frames up to the maximum UDP payload size minus overhead (~65,200 bytes) could be rejected if they happen to exceed 65535.",
      "fix_suggestion": "** Remove the 65535 limit since line 912 already validates against the actual packet bounds.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-015",
      "title": "Stream State Machine Allows Unexpected Transitions from OPEN",
      "category": "correctness",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Bidirectional streams cannot properly half-close, which would break applications that shut down one direction while keeping the other open.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/stream.c"
        ],
        "symbols": [
          "DATA_SENT",
          "RECV",
          "RESET_RECVD",
          "RESET_SENT",
          "SEND",
          "SIZE_KNOWN"
        ],
        "line_ranges": [],
        "snippets": [
          "case TQUIC_STREAM_OPEN:\n    /* Can go to SIZE_KNOWN, DATA_SENT, RESET_SENT, RESET_RECVD */\n    if (new_state != TQUIC_STREAM_SIZE_KNOWN &&\n        new_state != TQUIC_STREAM_DATA_SENT &&\n        new_state != TQUIC_STREAM_RESET_SENT &&\n        new_state != TQUIC_STREAM_RESET_RECVD)\n        return -EINVAL;\n    break;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Per RFC 9000 Figure 3 (bidirectional stream states), OPEN can transition to SEND (half-close receiving) or RECV (half-close sending), SIZE_KNOWN (recv side), DATA_SENT (send side), RESET_SENT (send side), or RESET_RECVD (recv side). The current implementation is missing SEND and RECV transitions from OPEN.",
      "fix_suggestion": "** Add TQUIC_STREAM_SEND and TQUIC_STREAM_RECV as valid transitions from OPEN.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-016",
      "title": "`ext->final_size = -1` Uses Signed Overflow",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** LOW - functionally correct but poor code clarity. Any comparison against 2^62-1 limits would pass incorrectly if the sentinel is not properly handled.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/core/stream.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [
          "ext->final_size = -1;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `final_size` is likely a `u64` field. Assigning `-1` produces `U64_MAX` (0xFFFFFFFFFFFFFFFF). While this is a common sentinel pattern, RFC 9000 limits stream data offsets to 2^62-1. Using U64_MAX as \"unknown\" is correct but should use a named constant like `TQUIC_STREAM_SIZE_UNKNOWN` to avoid confusion.",
      "fix_suggestion": "** Define `#define TQUIC_STREAM_SIZE_UNKNOWN U64_MAX` and use it consistently.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-017",
      "title": "Buffer Overflow in ClientHello Extension Building",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "** A stack or heap buffer overflow, exploitable by triggering specific extension combinations. An attacker who controls which extensions are negotiated (e.g., via a malicious server that triggers retry with specific parameters) could achieve remote code execution in kernel context.\n- **Code Pattern:**\n  ```c\n  // Extensions are written sequentially without cumulative bounds checking\n  // Each individual write may check, but total is not validated\n  ```\n- **Recommendation:** Add a running `offset` tracker and validate `offset + needed_bytes <= buf_len` before every write operation. Return `-ENOSPC` if insufficient space.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/handshake.c"
        ],
        "symbols": [
          "enabled",
          "negotiated",
          "tquic_hs_build_ch_extensions"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The function `tquic_hs_build_ch_extensions()` writes extension data into `buf` without consistently checking against `buf_len`. When multiple extensions are enabled (supported_versions, key_share, ALPN, SNI, transport parameters, PSK), the cumulative writes can overflow the provided buffer.\n- **Impact:** A stack or heap buffer overflow, exploitable by triggering specific extension combinations. An attacker who controls which extensions are negotiated (e.g., via a malicious server that triggers retry with specific parameters) could achieve remote code execution in kernel context.\n- **Code Pattern:**\n  ```c\n  // Extensions are written sequentially without cumulative bounds checking\n  // Each individual write may check, but total is not validated\n  ```\n- **Recommendation:** Add a running `offset` tracker and validate `offset + needed_bytes <= buf_len` before every write operation. Return `-ENOSPC` if insufficient space.",
      "fix_suggestion": "** Add a running `offset` tracker and validate `offset + needed_bytes <= buf_len` before every write operation. Return `-ENOSPC` if insufficient space.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-018",
      "title": "Stack Buffer Overflow in HKDF-Expand-Label (handshake.c)",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Kernel stack buffer overflow, potentially leading to stack smashing and kernel code execution. The handshake context processes data from untrusted peers.\n- **Recommendation:** Add explicit bounds check: `if (label_len + context_len + 10 > sizeof(hkdf_label)) return -EINVAL;` before any writes to the buffer. Note: the zero_rtt.c implementation at line ~238 correctly has this check (`if (label_len > 245 || context_len > 245 || (10 + label_len + context_len) > sizeof(hkdf_label)) return -EINVAL;`).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/handshake.c"
        ],
        "symbols": [
          "Label",
          "check"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The function uses a 512-byte stack buffer `hkdf_label` but does not adequately validate the combined size of `label_len + context_len` before writing into it. While TLS 1.3 label lengths are typically small, an attacker-controlled label from a malicious peer could exceed the buffer.\n- **Impact:** Kernel stack buffer overflow, potentially leading to stack smashing and kernel code execution. The handshake context processes data from untrusted peers.\n- **Recommendation:** Add explicit bounds check: `if (label_len + context_len + 10 > sizeof(hkdf_label)) return -EINVAL;` before any writes to the buffer. Note: the zero_rtt.c implementation at line ~238 correctly has this check (`if (label_len > 245 || context_len > 245 || (10 + label_len + context_len) > sizeof(hkdf_label)) return -EINVAL;`).",
      "fix_suggestion": "** Add explicit bounds check: `if (label_len + context_len + 10 > sizeof(hkdf_label)) return -EINVAL;` before any writes to the buffer. Note: the zero_rtt.c implementation at line ~238 correctly has this check (`if (label_len > 245 || context_len > 245 || (10 + label_len + context_len) > sizeof(hkdf_label)) return -EINVAL;`).",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-019",
      "title": "Fragile Hardcoded Offset for Key Update State Access",
      "category": "correctness",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Reading arbitrary memory as a pointer, leading to dereferencing wild pointers. This is a latent kernel memory corruption vulnerability triggered by any structure layout change (e.g., adding a field, changing compiler, changing config options that affect padding).\n- **Code Pattern:**\n  ```c\n  struct tquic_key_update_state *tquic_crypto_get_key_update_state(void *crypto_state)\n  {\n      // Hardcoded offset 304 bytes into opaque structure\n      return *(struct tquic_key_update_state **)((u8 *)crypto_state + 304);\n  }\n  ```\n- **Recommendation:** Use a proper typed structure with a named field, or use `container_of()` macro. Never use raw byte offsets to access structure members. Define a proper interface header that both the crypto state creator and this function share.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/key_update.c"
        ],
        "symbols": [
          "change",
          "container_of",
          "offset",
          "tquic_crypto_get_key_update_state"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The function `tquic_crypto_get_key_update_state()` uses a hardcoded byte offset (304 bytes) to access the `key_update` pointer from an opaque `void *crypto_state`. This is extremely fragile and will silently read the wrong memory if the crypto_state structure layout changes.\n- **Impact:** Reading arbitrary memory as a pointer, leading to dereferencing wild pointers. This is a latent kernel memory corruption vulnerability triggered by any structure layout change (e.g., adding a field, changing compiler, changing config options that affect padding).\n- **Code Pattern:**\n  ```c\n  struct tquic_key_update_state *tquic_crypto_get_key_update_state(void *crypto_state)\n  {\n      // Hardcoded offset 304 bytes into opaque structure\n      return *(struct tquic_key_update_state **)((u8 *)crypto_state + 304);\n  }\n  ```\n- **Recommendation:** Use a proper typed structure with a named field, or use `container_of()` macro. Never use raw byte offsets to access structure members. Define a proper interface header that both the crypto state creator and this function share.",
      "fix_suggestion": "** Use a proper typed structure with a named field, or use `container_of()` macro. Never use raw byte offsets to access structure members. Define a proper interface header that both the crypto state creator and this function share.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-020",
      "title": "Per-Call crypto_aead_setkey in Encrypt/Decrypt Hot Path",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Beyond the performance impact, calling `setkey` repeatedly may interact poorly with hardware crypto accelerators that cache expanded keys. Some implementations may leak timing information if the key schedule is not constant-time. Additionally, if the crypto driver implementation is not reentrant for concurrent setkey calls on a shared tfm, this creates a race condition.\n- **Recommendation:** Set the key once when it changes (at key installation time), not on every packet. Store the AEAD transform with the key pre-set in `tquic_key_generation`.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/tls.c"
        ],
        "symbols": [
          "changes",
          "crypto_aead_setkey",
          "expansion"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `crypto_aead_setkey()` is called on every encrypt and decrypt operation. In the kernel crypto API, `setkey` may trigger key expansion (AES key schedule), which involves non-trivial computation and potentially sleeping allocations.\n- **Impact:** Beyond the performance impact, calling `setkey` repeatedly may interact poorly with hardware crypto accelerators that cache expanded keys. Some implementations may leak timing information if the key schedule is not constant-time. Additionally, if the crypto driver implementation is not reentrant for concurrent setkey calls on a shared tfm, this creates a race condition.\n- **Recommendation:** Set the key once when it changes (at key installation time), not on every packet. Store the AEAD transform with the key pre-set in `tquic_key_generation`.",
      "fix_suggestion": "** Set the key once when it changes (at key installation time), not on every packet. Store the AEAD transform with the key pre-set in `tquic_key_generation`.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-021",
      "title": "Per-Call crypto_alloc_aead in 0-RTT Encrypt/Decrypt",
      "category": "perf",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** `crypto_alloc_aead()` performs module loading, memory allocation, and initialization. Under load, this will cause significant latency spikes and memory pressure. Under memory pressure, GFP_ATOMIC allocations may fail, causing packet drops.\n- **Recommendation:** Pre-allocate the AEAD transform during `tquic_zero_rtt_init()` or `tquic_zero_rtt_attempt()` and reuse it for the lifetime of the 0-RTT state.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/zero_rtt.c"
        ],
        "symbols": [
          "crypto_alloc_aead",
          "tquic_zero_rtt_attempt",
          "tquic_zero_rtt_decrypt",
          "tquic_zero_rtt_encrypt",
          "tquic_zero_rtt_init"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Both `tquic_zero_rtt_encrypt()` and `tquic_zero_rtt_decrypt()` call `crypto_alloc_aead()` on every invocation. This allocates a new crypto transform, sets the key, performs the operation, then frees it. This is extremely expensive in the hot path.\n- **Impact:** `crypto_alloc_aead()` performs module loading, memory allocation, and initialization. Under load, this will cause significant latency spikes and memory pressure. Under memory pressure, GFP_ATOMIC allocations may fail, causing packet drops.\n- **Recommendation:** Pre-allocate the AEAD transform during `tquic_zero_rtt_init()` or `tquic_zero_rtt_attempt()` and reuse it for the lifetime of the 0-RTT state.",
      "fix_suggestion": "** Pre-allocate the AEAD transform during `tquic_zero_rtt_init()` or `tquic_zero_rtt_attempt()` and reuse it for the lifetime of the 0-RTT state.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-022",
      "title": "Custom ASN.1 Parser - High Attack Surface",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** While the current parser appears to have basic bounds checking, the complexity of X.509 parsing means subtle bugs are likely. Historical CVEs in ASN.1 parsers (across all implementations) demonstrate this is a high-risk area.\n- **Recommendation:** Consider using the kernel's built-in ASN.1 decoder (`lib/asn1_decoder.c`) and the x509 certificate parser (`crypto/asymmetric_keys/x509_cert_parser.c`) which have been battle-tested. If the custom parser must be retained, add fuzzing tests targeting all parsing entry points.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "decoder",
          "infrastructure",
          "parser",
          "parsers"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The certificate verification module implements a custom ASN.1/DER parser rather than using the kernel's existing ASN.1 infrastructure (`lib/asn1_decoder.c`). Custom parsers for complex formats like ASN.1 are a known source of security vulnerabilities. The parser processes attacker-controlled certificate data from the network.\n- **Impact:** While the current parser appears to have basic bounds checking, the complexity of X.509 parsing means subtle bugs are likely. Historical CVEs in ASN.1 parsers (across all implementations) demonstrate this is a high-risk area.\n- **Recommendation:** Consider using the kernel's built-in ASN.1 decoder (`lib/asn1_decoder.c`) and the x509 certificate parser (`crypto/asymmetric_keys/x509_cert_parser.c`) which have been battle-tested. If the custom parser must be retained, add fuzzing tests targeting all parsing entry points.",
      "fix_suggestion": "** Consider using the kernel's built-in ASN.1 decoder (`lib/asn1_decoder.c`) and the x509 certificate parser (`crypto/asymmetric_keys/x509_cert_parser.c`) which have been battle-tested. If the custom parser must be retained, add fuzzing tests targeting all parsing entry points.",
      "tests_to_add": [
        "**Recommendation:** Consider using the kernel's built-in ASN.1 decoder (`lib/asn1_decoder.c`) and the x509 certificate parser (`crypto/asymmetric_keys/x509_cert_parser.c`) which have been battle-tested. If the custom parser must be retained, add fuzzing tests targeting all parsing entry points."
      ],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-023",
      "title": "OCSP Stapling Accepted Without Verification",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** An attacker with a revoked certificate can include arbitrary OCSP stapling data to bypass revocation checking. This completely defeats the purpose of revocation checking when OCSP stapling is present.\n- **Code Snippet:**\n  ```c\n  if (ctx->ocsp_stapling && ctx->ocsp_stapling_len > 0) {\n      /* OCSP response parsing would go here. */\n      pr_debug(\"tquic_cert: OCSP stapling present (%u bytes)\\n\",\n               ctx->ocsp_stapling_len);\n      return 0;  // Accepts without verification!\n  }\n  ```\n- **Recommendation:** Either implement OCSP response verification or remove the early return so that the \"no OCSP available\" path is taken, which at least logs warnings in hard-fail mode.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "pr_debug",
          "present"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** When OCSP stapling data is present, the function immediately returns success without parsing or verifying the OCSP response. The comment says \"For now, accept stapled responses as valid.\"\n- **Impact:** An attacker with a revoked certificate can include arbitrary OCSP stapling data to bypass revocation checking. This completely defeats the purpose of revocation checking when OCSP stapling is present.\n- **Code Snippet:**\n  ```c\n  if (ctx->ocsp_stapling && ctx->ocsp_stapling_len > 0) {\n      /* OCSP response parsing would go here. */\n      pr_debug(\"tquic_cert: OCSP stapling present (%u bytes)\\n\",\n               ctx->ocsp_stapling_len);\n      return 0;  // Accepts without verification!\n  }\n  ```\n- **Recommendation:** Either implement OCSP response verification or remove the early return so that the \"no OCSP available\" path is taken, which at least logs warnings in hard-fail mode.",
      "fix_suggestion": "** Either implement OCSP response verification or remove the early return so that the \"no OCSP available\" path is taken, which at least logs warnings in hard-fail mode.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-024",
      "title": "Race Condition in Key Update Secret Installation",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** A concurrent reader could attempt to use secrets for which AEAD keys have not yet been derived, leading to use of zero-initialized or stale key material. This could result in plaintext exposure or authentication bypass.\n- **Recommendation:** Use a state flag (e.g., `keys_installing`) that prevents concurrent use during the derivation window. Set the flag under the first lock acquisition, derive keys, then clear it under the second lock acquisition.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/key_update.c"
        ],
        "symbols": [
          "derivation",
          "flag",
          "state"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The function installs secrets under the spinlock, then drops the lock to perform key derivation (which may sleep), then re-acquires the lock to update state. Between the lock release and re-acquisition, another thread could observe partially-installed state (secrets set but keys not yet derived).\n- **Impact:** A concurrent reader could attempt to use secrets for which AEAD keys have not yet been derived, leading to use of zero-initialized or stale key material. This could result in plaintext exposure or authentication bypass.\n- **Recommendation:** Use a state flag (e.g., `keys_installing`) that prevents concurrent use during the derivation window. Set the flag under the first lock acquisition, derive keys, then clear it under the second lock acquisition.",
      "fix_suggestion": "** Use a state flag (e.g., `keys_installing`) that prevents concurrent use during the derivation window. Set the flag under the first lock acquisition, derive keys, then clear it under the second lock acquisition.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-025",
      "title": "Client Certificate Verification Uses Server Logic",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Client certificates with only clientAuth EKU would be rejected. Conversely, certificates with only serverAuth EKU would be incorrectly accepted for client authentication.\n- **Code Snippet:**\n  ```c\n  int tquic_hs_verify_client_cert(struct tquic_handshake *hs,\n                                  struct tquic_connection *conn)\n  {\n      return tquic_hs_verify_server_cert(hs, conn);\n  }\n  ```\n- **Recommendation:** Add a `bool is_server` parameter to the internal `verify_chain()` call path, or refactor so that client cert verification passes `is_server=false`.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "tquic_hs_verify_client_cert",
          "tquic_hs_verify_server_cert",
          "verify_chain"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_hs_verify_client_cert()` directly calls `tquic_hs_verify_server_cert()`, which always passes `true` for `is_server` in the chain verification. This means client certificates are checked for serverAuth EKU instead of clientAuth EKU.\n- **Impact:** Client certificates with only clientAuth EKU would be rejected. Conversely, certificates with only serverAuth EKU would be incorrectly accepted for client authentication.\n- **Code Snippet:**\n  ```c\n  int tquic_hs_verify_client_cert(struct tquic_handshake *hs,\n                                  struct tquic_connection *conn)\n  {\n      return tquic_hs_verify_server_cert(hs, conn);\n  }\n  ```\n- **Recommendation:** Add a `bool is_server` parameter to the internal `verify_chain()` call path, or refactor so that client cert verification passes `is_server=false`.",
      "fix_suggestion": "** Add a `bool is_server` parameter to the internal `verify_chain()` call path, or refactor so that client cert verification passes `is_server=false`.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-026",
      "title": "Transcript Buffer Not Zeroized Before Free",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** After `kfree()`, the transcript data remains in the slab cache and could be read by a local attacker with kernel memory access.\n- **Recommendation:** Use `kfree_sensitive()` or call `memzero_explicit()` on the transcript buffer before freeing.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/handshake.c"
        ],
        "symbols": [
          "kfree",
          "kfree_sensitive",
          "memzero_explicit"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The handshake cleanup function zeroizes most cryptographic secrets but does not zeroize the transcript buffer before freeing it. The transcript contains the full handshake, including encrypted data and potentially sensitive parameters.\n- **Impact:** After `kfree()`, the transcript data remains in the slab cache and could be read by a local attacker with kernel memory access.\n- **Recommendation:** Use `kfree_sensitive()` or call `memzero_explicit()` on the transcript buffer before freeing.",
      "fix_suggestion": "** Use `kfree_sensitive()` or call `memzero_explicit()` on the transcript buffer before freeing.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-027",
      "title": "Bloom Filter False Negatives Allow Replay",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** A replay attack timed to coincide with bloom filter rotation could succeed. The window is TTL/2 (30 minutes by default).\n- **Recommendation:** This is an inherent limitation of bloom filters. Document this as a known limitation. Consider augmenting with a small exact-match cache for recent tickets (last N tickets stored exactly) to eliminate the rotation window for the most common case.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/zero_rtt.c"
        ],
        "symbols": [
          "detected",
          "tickets"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The bloom filter anti-replay mechanism is probabilistic. With a 64K-bit filter and 4 hash functions, the false positive rate is reasonable, but rotation clears half the filter, creating a window where replays are not detected (false negatives during the rotation boundary).\n- **Impact:** A replay attack timed to coincide with bloom filter rotation could succeed. The window is TTL/2 (30 minutes by default).\n- **Recommendation:** This is an inherent limitation of bloom filters. Document this as a known limitation. Consider augmenting with a small exact-match cache for recent tickets (last N tickets stored exactly) to eliminate the rotation window for the most common case.",
      "fix_suggestion": "** This is an inherent limitation of bloom filters. Document this as a known limitation. Consider augmenting with a small exact-match cache for recent tickets (last N tickets stored exactly) to eliminate the rotation window for the most common case.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-028",
      "title": "Per-Call crypto_alloc_shash in Stateless Reset Token Generation",
      "category": "security",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** An attacker sending many short packets to trigger stateless reset processing could cause memory pressure through repeated crypto_alloc_shash calls.\n- **Recommendation:** Pre-allocate a per-CPU or global HMAC transform and reuse it.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_stateless_reset.c"
        ],
        "symbols": [
          "tquic_stateless_reset_generate_token"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_stateless_reset_generate_token()` allocates a new `crypto_shash` transform on every call. Under a stateless reset flood attack, this creates excessive memory allocation pressure.\n- **Impact:** An attacker sending many short packets to trigger stateless reset processing could cause memory pressure through repeated crypto_alloc_shash calls.\n- **Recommendation:** Pre-allocate a per-CPU or global HMAC transform and reuse it.",
      "fix_suggestion": "** Pre-allocate a per-CPU or global HMAC transform and reuse it.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-029",
      "title": "RSA-PSS Hash Algorithm Always Defaults to SHA-256",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Certificates using RSA-PSS with SHA-384 or SHA-512 will have incorrect signature verification (wrong hash computed over TBSCertificate), causing valid certificates to be rejected.\n- **Recommendation:** Parse the RSA-PSS AlgorithmIdentifier parameters to extract the actual hash algorithm.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "verification"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** When an RSA-PSS signature algorithm OID is detected, the hash algorithm always defaults to SHA-256 without parsing the RSA-PSS parameters that specify the actual hash algorithm.\n- **Impact:** Certificates using RSA-PSS with SHA-384 or SHA-512 will have incorrect signature verification (wrong hash computed over TBSCertificate), causing valid certificates to be rejected.\n- **Recommendation:** Parse the RSA-PSS AlgorithmIdentifier parameters to extract the actual hash algorithm.",
      "fix_suggestion": "** Parse the RSA-PSS AlgorithmIdentifier parameters to extract the actual hash algorithm.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-030",
      "title": "Time Parsing Does Not Validate Digit Characters",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** A malformed certificate with non-digit time values could cause incorrect validity period computation, potentially allowing expired or not-yet-valid certificates to pass validation.\n- **Recommendation:** Add `isdigit()` checks for all time component characters before conversion.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "isdigit",
          "parse_time",
          "subtraction"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The `parse_time()` function converts ASCII digit characters to integers using subtraction (`t[0] - '0'`) without verifying the characters are actually digits. Non-digit characters would produce incorrect values without detection.\n- **Impact:** A malformed certificate with non-digit time values could cause incorrect validity period computation, potentially allowing expired or not-yet-valid certificates to pass validation.\n- **Recommendation:** Add `isdigit()` checks for all time component characters before conversion.",
      "fix_suggestion": "** Add `isdigit()` checks for all time component characters before conversion.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-031",
      "title": "Missing Bounds Check on tbs Pointer in Signature Parse",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** An integer underflow on `remaining` passed to `parse_signature()` could cause out-of-bounds reads during signature parsing.\n- **Recommendation:** Validate that `cert->tbs + cert->tbs_len <= data + total_len` before computing `remaining`.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "buffer",
          "parse_signature",
          "parse_tbs_certificate"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** After `parse_tbs_certificate()`, the code computes `after_tbs = cert->tbs + cert->tbs_len` and `remaining = content_len - (after_tbs - p)`. The `cert->tbs` pointer points into the original `data` buffer (not the copy in `cert->raw`). If `tbs_len` exceeds `content_len`, the subtraction wraps to a large value.\n- **Impact:** An integer underflow on `remaining` passed to `parse_signature()` could cause out-of-bounds reads during signature parsing.\n- **Recommendation:** Validate that `cert->tbs + cert->tbs_len <= data + total_len` before computing `remaining`.",
      "fix_suggestion": "** Validate that `cert->tbs + cert->tbs_len <= data + total_len` before computing `remaining`.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-032",
      "title": "EKU Request ID Increment Outside Lock",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** While not directly exploitable, this creates a TOCTOU window where two concurrent callers may get sequential IDs but their requests are not guaranteed to be enqueued in order. This is a correctness issue rather than a direct security vulnerability.\n- **Recommendation:** Keep the lock held through request allocation and insertion, or use an atomic increment for the request ID.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/extended_key_update.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The `next_request_id` is incremented under the lock, but then the lock is dropped before the request is fully constructed and re-inserted. Between the unlock at line 440 and re-lock at line 451, another thread could also increment `next_request_id`, potentially causing request ID ordering issues.\n- **Impact:** While not directly exploitable, this creates a TOCTOU window where two concurrent callers may get sequential IDs but their requests are not guaranteed to be enqueued in order. This is a correctness issue rather than a direct security vulnerability.\n- **Recommendation:** Keep the lock held through request allocation and insertion, or use an atomic increment for the request ID.",
      "fix_suggestion": "** Keep the lock held through request allocation and insertion, or use an atomic increment for the request ID.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-033",
      "title": "QAT Encrypt Sets Key on Every Call",
      "category": "perf",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Negates the performance benefit of hardware offload by adding per-packet key setup overhead.\n- **Recommendation:** Set the key once during context initialization and only re-set on key update.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/hw_offload.c"
        ],
        "symbols": [
          "crypto_aead_setkey",
          "tquic_qat_encrypt"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_qat_encrypt()` calls `crypto_aead_setkey()` on every encryption operation. For QAT hardware, key setup involves sending the key to the hardware accelerator, which is expensive.\n- **Impact:** Negates the performance benefit of hardware offload by adding per-packet key setup overhead.\n- **Recommendation:** Set the key once during context initialization and only re-set on key update.",
      "fix_suggestion": "** Set the key once during context initialization and only re-set on key update.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-034",
      "title": "Shared Exporter and Resumption Secrets Not Zeroized",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Key material persists in kernel memory longer than necessary, increasing the window for extraction via kernel memory vulnerabilities.\n- **Recommendation:** Zeroize these secrets using `memzero_explicit()` as soon as they have been consumed (after key derivation and ticket issuance).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/handshake.c"
        ],
        "symbols": [
          "consumed",
          "memzero_explicit"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The `exporter_secret` and `resumption_secret` derived during the handshake are stored for later use but are not zeroized in the cleanup path after they are no longer needed. These secrets could enable session hijacking if leaked.\n- **Impact:** Key material persists in kernel memory longer than necessary, increasing the window for extraction via kernel memory vulnerabilities.\n- **Recommendation:** Zeroize these secrets using `memzero_explicit()` as soon as they have been consumed (after key derivation and ticket issuance).",
      "fix_suggestion": "** Zeroize these secrets using `memzero_explicit()` as soon as they have been consumed (after key derivation and ticket issuance).",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-035",
      "title": "Self-Signed Certificate Check Uses memcmp Instead of crypto_memneq",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** The timing difference is minor for DN comparison (not a secret), but the inconsistency suggests potential for similar mistakes in security-critical comparisons.\n- **Recommendation:** Use `crypto_memneq()` consistently for all comparisons, or use `memcmp()` consistently for non-secret data. The key point is to be consistent and use constant-time comparison for any data whose equality/inequality should not leak timing information.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "comparison",
          "crypto_memneq",
          "memcmp",
          "tquic_x509_cert_parse",
          "tquic_x509_verify_signature"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The self-signed detection in `tquic_x509_verify_signature()` uses `memcmp()` for comparing issuer and subject DNs, while the same check in `tquic_x509_cert_parse()` at line 1628 correctly uses `crypto_memneq()`. Inconsistent use of constant-time comparison.\n- **Impact:** The timing difference is minor for DN comparison (not a secret), but the inconsistency suggests potential for similar mistakes in security-critical comparisons.\n- **Recommendation:** Use `crypto_memneq()` consistently for all comparisons, or use `memcmp()` consistently for non-secret data. The key point is to be consistent and use constant-time comparison for any data whose equality/inequality should not leak timing information.",
      "fix_suggestion": "** Use `crypto_memneq()` consistently for all comparisons, or use `memcmp()` consistently for non-secret data. The key point is to be consistent and use constant-time comparison for any data whose equality/inequality should not leak timing information.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-036",
      "title": "Unused HKDF-Expand Output in Extended Key Update",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": "** The PSK injection feature does not actually mix PSK material into the key derivation. The resulting keys are identical regardless of whether a PSK is injected, defeating the purpose of the feature.\n- **Recommendation:** Actually use the `mixed_secret` as input to the subsequent key derivation rather than the standard derivation path.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/extended_key_update.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** When PSK is included, the function derives a `mixed_secret` via HKDF-Extract but then zeroizes it immediately without actually using it in the key derivation. The comment says \"Now we need to use this mixed_secret\" but the code does not actually do so.\n- **Impact:** The PSK injection feature does not actually mix PSK material into the key derivation. The resulting keys are identical regardless of whether a PSK is injected, defeating the purpose of the feature.\n- **Recommendation:** Actually use the `mixed_secret` as input to the subsequent key derivation rather than the standard derivation path.",
      "fix_suggestion": "** Actually use the `mixed_secret` as input to the subsequent key derivation rather than the standard derivation path.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-037",
      "title": "Header Protection Mask Not Zeroized in All Error Paths",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Minimal - mask data is not directly sensitive (it is derived from public packet data), but defense-in-depth suggests cleanup.\n- **Recommendation:** Ensure `memzero_explicit()` is called on the mask buffer in all return paths, including error paths.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/header_protection.c"
        ],
        "symbols": [
          "memzero_explicit",
          "sensitive"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** While the main code path properly zeroizes the mask buffer after use, some error return paths may leave partial mask data on the stack.\n- **Impact:** Minimal - mask data is not directly sensitive (it is derived from public packet data), but defense-in-depth suggests cleanup.\n- **Recommendation:** Ensure `memzero_explicit()` is called on the mask buffer in all return paths, including error paths.",
      "fix_suggestion": "** Ensure `memzero_explicit()` is called on the mask buffer in all return paths, including error paths.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-038",
      "title": "Per-CPU Stats Not Protected Against Torn Reads on 32-bit",
      "category": "security",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Statistics may show incorrect values on 32-bit systems. This is an informational issue with no security impact.\n- **Recommendation:** Use `u64_stats_sync` infrastructure for proper 64-bit stats on 32-bit architectures.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/hw_offload.c"
        ],
        "symbols": [
          "READ_ONCE"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The 64-bit per-CPU statistics are read with `READ_ONCE()` but on 32-bit architectures, a 64-bit read is not atomic and can produce torn values.\n- **Impact:** Statistics may show incorrect values on 32-bit systems. This is an informational issue with no security impact.\n- **Recommendation:** Use `u64_stats_sync` infrastructure for proper 64-bit stats on 32-bit architectures.",
      "fix_suggestion": "** Use `u64_stats_sync` infrastructure for proper 64-bit stats on 32-bit architectures.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-039",
      "title": "Procfs trusted_cas File Writable Without Capability Check",
      "category": "security",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Any process running as root can modify the trusted CA store. While root has broad permissions, adding a capability check provides defense-in-depth.\n- **Recommendation:** Add `capable(CAP_NET_ADMIN)` check at the start of `tquic_proc_trusted_cas_write()`.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "capable",
          "check",
          "tquic_proc_trusted_cas_write"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The `/proc/net/tquic_cert/trusted_cas` file is created with mode 0644, making it writable by root. However, there is no explicit capability check (e.g., `CAP_NET_ADMIN`) in the write handler.\n- **Impact:** Any process running as root can modify the trusted CA store. While root has broad permissions, adding a capability check provides defense-in-depth.\n- **Recommendation:** Add `capable(CAP_NET_ADMIN)` check at the start of `tquic_proc_trusted_cas_write()`.",
      "fix_suggestion": "** Add `capable(CAP_NET_ADMIN)` check at the start of `tquic_proc_trusted_cas_write()`.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-040",
      "title": "Module Parameters Expose Security Configuration",
      "category": "security",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Defense-in-depth concern. Root can already bypass most security, but making it easy to silently disable cert verification is risky.\n- **Recommendation:** Log a prominent warning when verification mode is set to NONE. Consider requiring a special flag to disable verification in production builds.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Security-critical parameters like `tquic_cert_verify_mode` and `tquic_cert_revocation_mode` are controlled via sysctl. A privileged attacker could disable certificate verification entirely by setting `tquic_cert_verify_mode = 0`.\n- **Impact:** Defense-in-depth concern. Root can already bypass most security, but making it easy to silently disable cert verification is risky.\n- **Recommendation:** Log a prominent warning when verification mode is set to NONE. Consider requiring a special flag to disable verification in production builds.",
      "fix_suggestion": "** Log a prominent warning when verification mode is set to NONE. Consider requiring a special flag to disable verification in production builds.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-041",
      "title": "Batch Crypto Allocates Per-Packet Temporary Buffer",
      "category": "perf",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Performance issue. Memory allocation per packet in the batch path adds latency and memory pressure.\n- **Recommendation:** Perform in-place encryption if the caller's buffer has sufficient space (the `data_buf_len` check already exists), or pre-allocate a shared temporary buffer.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/hw_offload.c"
        ],
        "symbols": [
          "space"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The batch encryption function allocates a temporary `ct_buf` via `kmalloc(GFP_ATOMIC)` for each packet in the batch, then copies the result back. This defeats the purpose of batch processing.\n- **Impact:** Performance issue. Memory allocation per packet in the batch path adds latency and memory pressure.\n- **Recommendation:** Perform in-place encryption if the caller's buffer has sufficient space (the `data_buf_len` check already exists), or pre-allocate a shared temporary buffer.",
      "fix_suggestion": "** Perform in-place encryption if the caller's buffer has sufficient space (the `data_buf_len` check already exists), or pre-allocate a shared temporary buffer.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-042",
      "title": "Key Update Timeout Revert Could Race With Concurrent Update",
      "category": "concurrency",
      "severity": "S3",
      "confidence": "low",
      "impact": "** The spinlock should prevent data corruption, but the logical state could be inconsistent (timeout reverts keys while the peer has already adopted the new keys).\n- **Recommendation:** Add a generation counter or sequence number to the key update so the timeout handler can detect if a response arrived between the timeout firing and the lock acquisition.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/key_update.c"
        ],
        "symbols": [
          "inconsistent",
          "mechanism"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The key update timeout/revert mechanism (3*PTO timeout) could race with a legitimate late key phase acknowledgment from the peer. If the timeout fires just as the peer's response arrives, both paths may modify the key state concurrently.\n- **Impact:** The spinlock should prevent data corruption, but the logical state could be inconsistent (timeout reverts keys while the peer has already adopted the new keys).\n- **Recommendation:** Add a generation counter or sequence number to the key update so the timeout handler can detect if a response arrived between the timeout firing and the lock acquisition.",
      "fix_suggestion": "** Add a generation counter or sequence number to the key update so the timeout handler can detect if a response arrived between the timeout firing and the lock acquisition.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-043",
      "title": "Certificate Chain Length Limit Checked Late",
      "category": "concurrency",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Minor resource waste - one extra certificate is parsed and allocated before the limit check triggers. Not a security vulnerability since the limit is still enforced.\n- **Recommendation:** Move the chain length check to the beginning of the loop iteration.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/crypto/cert_verify.c"
        ],
        "symbols": [
          "CSPRNG",
          "check",
          "comparisons",
          "crypto_memneq",
          "get_random_bytes",
          "limits",
          "memzero_explicit",
          "record",
          "reviewed",
          "tquic_zero_rtt_decrypt"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The certificate chain parsing loop checks `TQUIC_MAX_CERT_CHAIN_LEN` at the end of the loop body, meaning it will parse and allocate one certificate beyond the limit before breaking.\n- **Impact:** Minor resource waste - one extra certificate is parsed and allocated before the limit check triggers. Not a security vulnerability since the limit is still enforced.\n- **Recommendation:** Move the chain length check to the beginning of the loop iteration.",
      "fix_suggestion": "** Move the chain length check to the beginning of the loop iteration.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-044",
      "title": "HIGH-01: Function Pointer Stored in skb->cb Without Validation",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": ": If `skb->cb` is corrupted (by another network layer or a bug), calling the stored function pointer leads to arbitrary code execution in kernel context.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c"
        ],
        "symbols": [
          "corrupted",
          "send_fn",
          "void"
        ],
        "line_ranges": [],
        "snippets": [
          "/* Store send function in skb->cb */\n*(unsigned long *)skb->cb = (unsigned long)send_fn;\n\n/* Later, retrieve and call */\nvoid (*send_fn)(struct sk_buff *) =\n    (void (*)(struct sk_buff *))(*(unsigned long *)skb->cb);\nsend_fn(skb);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": Function pointers are stored in `skb->cb` (the control buffer) and later retrieved and called without validation. The `skb->cb` area is 48 bytes and is used by multiple layers; if any intermediate processing corrupts or overwrites `cb[0]`, an attacker could achieve code execution. The check `if (skb->cb[0])` at line 71 treats cb[0] as a boolean but the actual pointer starts at `*(unsigned long *)skb->cb` -- a nonzero cb[0] byte does not guarantee a valid function pointer. Furthermore, the decoy traffic code at line 533 writes `'D', 'E', 'C', 'O', 'Y'` into cb, which would be interpreted as a garbage function pointer if the SKB were accidentally passed through the send path.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-045",
      "title": "HIGH-02: No CAP_NET_ADMIN Check for Tunnel Creation",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": ": Unprivileged users can open arbitrary TCP connections via the tunnel mechanism, potentially circumventing local firewall rules and creating unauthorized network flows.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_tunnel.c"
        ],
        "symbols": [
          "capable",
          "destinations",
          "tquic_tunnel_create",
          "tquic_tunnel_create_tproxy"
        ],
        "line_ranges": [],
        "snippets": [
          "struct tquic_tunnel *tquic_tunnel_create(struct tquic_client *client,\n                                         struct tquic_stream *stream,\n                                         const u8 *header_data,\n                                         size_t header_len)"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": `tquic_tunnel_create()` and `tquic_tunnel_create_tproxy()` do not perform any capability checks. Any process that can open a QUIC stream can instruct the kernel to create outbound TCP connections to arbitrary destinations (subject to the incomplete SSRF filter). Tunnel creation should require `CAP_NET_ADMIN` or at minimum `CAP_NET_RAW`.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-046",
      "title": "HIGH-03: Load Balancer Encryption Key Not Zeroized on Destroy",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": ": Encryption key material remains in freed memory, potentially recoverable via heap spraying or other memory disclosure techniques.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/lb/quic_lb.c"
        ],
        "symbols": [
          "crypto_free_sync_skcipher",
          "kmem_cache_free",
          "memzero_explicit",
          "tquic_lb_config_destroy"
        ],
        "line_ranges": [],
        "snippets": [
          "void tquic_lb_config_destroy(struct tquic_lb_config *cfg)\n{\n    if (!cfg)\n        return;\n\n    if (cfg->aes_tfm)\n        crypto_free_sync_skcipher(cfg->aes_tfm);\n\n    kmem_cache_free(lb_config_cache, cfg);\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": The AES-128 encryption key stored in `cfg->encryption_key[16]` is not zeroized before the config structure is freed. The SLAB allocator may reuse this memory for other allocations, potentially exposing the key material through information disclosure vulnerabilities in other subsystems. The `server_id` is also not zeroized.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-047",
      "title": "HIGH-04: Constant-Time CID Validation Has Branching on Lengths",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": ": Potential out-of-bounds read if `cid_len != expected_len`. The timing variation from branches may leak length information.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c"
        ],
        "symbols": [
          "min",
          "tquic_ct_memcmp",
          "tquic_ct_validate_cid"
        ],
        "line_ranges": [],
        "snippets": [
          "bool tquic_ct_validate_cid(const u8 *cid, size_t cid_len,\n                           const u8 *expected, size_t expected_len)\n{\n    ...\n    /* Compare lengths without branching */\n    len_match = !(cid_len ^ expected_len);\n\n    /* Use the larger length to ensure constant-time */\n    max_len = cid_len > expected_len ? cid_len : expected_len;\n    if (max_len > sizeof(dummy_buf))\n        max_len = si"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": Despite the comment \"without branching,\" the ternary `cid_len > expected_len ? cid_len : expected_len` is a branch on secret data. The comparison `max_len > sizeof(dummy_buf)` is also a branch. More critically, `tquic_ct_memcmp(cid, expected, max_len)` reads `max_len` bytes from both `cid` and `expected`, but if `cid_len < max_len`, this is an out-of-bounds read. The function assumes both buffers are at least `max_len` bytes, which is not guaranteed.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-048",
      "title": "HIGH-05: Unbounded Connection Creation via Netlink",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": ": Denial of service via kernel memory exhaustion by creating millions of connection info objects.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_netlink.c"
        ],
        "symbols": [
          "connections",
          "tquic_conn_info_create",
          "tquic_conn_lookup",
          "tquic_nl_cmd_path_add"
        ],
        "line_ranges": [],
        "snippets": [
          "conn = tquic_conn_lookup(net, conn_id);\nif (!conn) {\n    conn = tquic_conn_info_create(net, conn_id);\n    if (!conn)\n        return -ENOMEM;\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": In `tquic_nl_cmd_path_add()`, a new connection is automatically created if the specified `conn_id` does not exist. There is no limit on the number of connections that can be created per namespace. While paths per connection are limited to 256, an attacker with `CAP_NET_ADMIN` can create unlimited connection objects by varying `conn_id`, leading to kernel memory exhaustion.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-049",
      "title": "HIGH-06: Timing Normalization Can Block in Packet Processing Path",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": ": Kernel panic if called from softirq context. Denial of service via intentional latency injection (attacker sends many packets, each causing 2ms delay on the receiver).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c"
        ],
        "symbols": [
          "context",
          "injection",
          "might_sleep",
          "msleep",
          "processing",
          "tquic_timing_normalize_process",
          "udelay",
          "usleep_range"
        ],
        "line_ranges": [],
        "snippets": [
          "if (delay_us <= 10) {\n    udelay(delay_us);\n} else if (delay_us <= 1000) {\n    usleep_range(delay_us, delay_us + 10);\n} else {\n    msleep(delay_us / 1000);\n    ...\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": `tquic_timing_normalize_process()` is called during incoming packet processing (`tquic_exfil_process_incoming`). The function uses `usleep_range()` and `msleep()` which sleep and can only be called from process context. If called from softirq/BH context (typical for incoming packet processing), this will cause a BUG or schedule-while-atomic panic. Even if called from process context, blocking for up to 2ms per packet at PARANOID level is a denial-of-service amplifier.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-050",
      "title": "MED-01: Decoy Packet Size Calculation Can Underflow",
      "category": "security",
      "severity": "S3",
      "confidence": "low",
      "impact": ": Potential large memory allocation attempt (though allocation will fail). If MTU is exactly 64, `rand_val % 1` = 0, so `decoy_size = 64`, which is correct. Values below 64 are the problem.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c"
        ],
        "symbols": [
          "alloc_skb",
          "arithmetic",
          "attempt",
          "tquic_traffic_shaper_set_mtu"
        ],
        "line_ranges": [],
        "snippets": [
          "decoy_size = 64 + (rand_val % (shaper->mtu - 64 + 1));",
          "if (mtu < 64) mtu = 64;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": If `shaper->mtu` is set to a value less than 64 (e.g., via `tquic_traffic_shaper_set_mtu()` with no lower bound check), `shaper->mtu - 64 + 1` wraps around to a very large number due to unsigned arithmetic (both are `u32`). This would produce a decoy_size of up to ~4GB, and `alloc_skb()` with such a size would fail, but the modulo of a random u32 by a near-u32-max value could still produce very large sizes.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-051",
      "title": "MED-06: Tunnel Port Allocation Unsigned Underflow",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": ": None (correctly handled by bounds check), but defense-in-depth could be improved.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_tunnel.c"
        ],
        "symbols": [
          "None",
          "correctly",
          "ntohs"
        ],
        "line_ranges": [],
        "snippets": [
          "bit = ntohs(port) - ntohs(client->port_range_start);\nif (bit >= TQUIC_PORTS_PER_CLIENT)\n    return;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": The variable `bit` is `unsigned long`. If `ntohs(port) < ntohs(client->port_range_start)`, the subtraction wraps to a very large positive value. The `>= TQUIC_PORTS_PER_CLIENT` check catches this case correctly (the wrapped value will be much larger than 1000), so the function returns safely. However, this relies on unsigned wrap behavior and would be clearer with an explicit check.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-052",
      "title": "LOW-01: Duplicate MODULE_DESCRIPTION/MODULE_LICENSE in quic_exfil.c",
      "category": "security",
      "severity": "S3",
      "confidence": "low",
      "impact": ": None (build warning only).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c"
        ],
        "symbols": [
          "MODULE_AUTHOR",
          "MODULE_DESCRIPTION",
          "MODULE_LICENSE",
          "Mitigation",
          "None"
        ],
        "line_ranges": [],
        "snippets": [
          "MODULE_DESCRIPTION(\"TQUIC QUIC-Exfil Mitigation (draft-iab-quic-exfil)\");\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Linux Foundation\");"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": The MODULE_DESCRIPTION, MODULE_LICENSE, and MODULE_AUTHOR macros are declared twice in the same file. This is a code quality issue that may cause build warnings.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-053",
      "title": "LOW-02: Netlink Events Do Not Include Timestamp",
      "category": "security",
      "severity": "S3",
      "confidence": "low",
      "impact": ": Reduced observability, no security impact.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_netlink.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": Netlink event notifications for path up/down/change/migration do not include a kernel timestamp. Userspace tools cannot determine the exact time of events, making debugging and correlation with other log sources difficult.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-054",
      "title": "LOW-03: Volatile Qualifiers in Constant-Time Functions May Be Insufficient",
      "category": "security",
      "severity": "S3",
      "confidence": "low",
      "impact": ": Timing side-channel may still be exploitable despite the `volatile` annotation, particularly on architectures with speculative execution.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/security/quic_exfil.c"
        ],
        "symbols": [
          "crypto_memneq"
        ],
        "line_ranges": [],
        "snippets": [
          "const volatile unsigned char *aa = a;\nconst volatile unsigned char *bb = b;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": The `volatile` qualifier prevents the compiler from optimizing away reads, but does not prevent speculative execution or microarchitectural side channels. Modern compilers and CPUs may still optimize the loop or execute it speculatively in ways that leak timing information. The kernel provides `crypto_memneq()` for this purpose, which uses architecture-specific barriers.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-055",
      "title": "LOW-04: Load Balancer Stack Buffers for Feistel Not Zeroized on Error",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": ": Minor information disclosure risk in specialized attack scenarios.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/lb/quic_lb.c"
        ],
        "symbols": [
          "memzero_explicit",
          "tquic_lb_decrypt_four_pass",
          "tquic_lb_encrypt_four_pass"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": The `left[16]`, `right[16]`, `tmp[16]`, and `round_out[16]` buffers in `tquic_lb_encrypt_four_pass()` and `tquic_lb_decrypt_four_pass()` are not zeroized on error return paths. While stack memory is less persistent than heap, in a kernel with KASAN or stack reuse, residual plaintext/key-derived data could be observable.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-056",
      "title": "LOW-05: Netlink Family Exported as EXPORT_SYMBOL_GPL",
      "category": "memory",
      "severity": "S3",
      "confidence": "low",
      "impact": ": Other kernel modules could send spoofed TQUIC events to userspace listeners.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_netlink.c"
        ],
        "symbols": [
          "EXPORT_SYMBOL_GPL",
          "bypass"
        ],
        "line_ranges": [],
        "snippets": [
          "EXPORT_SYMBOL_GPL(tquic_genl_family);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": ": Exporting the entire `genl_family` structure allows other modules to directly manipulate the netlink family or send events on its behalf. This is unusual; typically only specific helper functions are exported.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-057",
      "title": "Adaptive Scheduler CWND Underflow in Score Calculation",
      "category": "correctness",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Both `cwnd` and `bytes_in_flight` are `u32`. When `bytes_in_flight > cwnd` (which occurs transiently during congestion or loss recovery), the subtraction wraps around to a very large `u64` value. This corrupts the score calculation, potentially giving a failing path the highest score and routing all traffic to it -- exactly the wrong behavior during congestion.\n- **Recommendation:** Add an explicit check: `cwnd_avail = (path->cc.cwnd > path->cc.bytes_in_flight) ? path->cc.cwnd - path->cc.bytes_in_flight : 0;`",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/multipath/tquic_scheduler.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [
          "if (path->cc.cwnd == 0)\n    cwnd_score = 0;\nelse {\n    cwnd_avail = path->cc.cwnd - path->cc.bytes_in_flight;\n    cwnd_score = (cwnd_avail * 1000) / path->cc.cwnd;\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Add an explicit check: `cwnd_avail = (path->cc.cwnd > path->cc.bytes_in_flight) ? path->cc.cwnd - path->cc.bytes_in_flight : 0;`",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-058",
      "title": "Path Score Computation Can Overflow in Migration Target Selection",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** The score starts at 1000000 and is multiplied by 1000, bandwidth, (100-loss), (256-priority), and weight without overflow checks. For a path with high bandwidth (e.g., 10Gbps = 1250000000) and low RTT, the intermediate value `1000000 * 1000 / rtt * bandwidth` can overflow u64 (bandwidth * 1000000000 > 2^64).\n- **Recommendation:** Reorder operations to divide before multiplying, or cap intermediate values.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_migration.c"
        ],
        "symbols": [
          "bandwidth",
          "min",
          "u64"
        ],
        "line_ranges": [],
        "snippets": [
          "u64 score = 1000000;\nscore = score * 1000 / stats->rtt_smoothed;\nscore = (score * stats->bandwidth) >> 20;\nscore = score * (100 - min(loss_pct, 90ULL)) / 100;\nscore = score * (256 - path->priority) / 256;\nscore = score * path->weight;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Reorder operations to divide before multiplying, or cap intermediate values.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-059",
      "title": "QPACK Dynamic Table Duplicate: Use-After-Free via Lock Drop",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "** An attacker who can trigger concurrent QPACK encoder instruction processing (e.g., via multiple streams referencing the same dynamic table) can cause kernel memory corruption, leading to privilege escalation or denial of service.\n- **Recommendation:** Either (a) increment the entry's refcount before dropping the lock, or (b) copy the name/value data into a local buffer before dropping the lock, or (c) use `GFP_ATOMIC` allocation under the lock (acceptable for small allocations).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/qpack_dynamic.c"
        ],
        "symbols": [
          "Either",
          "lock",
          "processing",
          "qpack_dynamic_table_duplicate"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `qpack_dynamic_table_duplicate()` looks up a source entry pointer under the table spinlock, then drops the lock to perform a `kmalloc(GFP_KERNEL)` allocation, then re-acquires the lock and uses the original source pointer. Between the lock drop and re-acquisition, another thread can evict the source entry from the dynamic table, freeing it. The subsequent `memcpy` from the stale pointer is a use-after-free.\n- **Impact:** An attacker who can trigger concurrent QPACK encoder instruction processing (e.g., via multiple streams referencing the same dynamic table) can cause kernel memory corruption, leading to privilege escalation or denial of service.\n- **Recommendation:** Either (a) increment the entry's refcount before dropping the lock, or (b) copy the name/value data into a local buffer before dropping the lock, or (c) use `GFP_ATOMIC` allocation under the lock (acceptable for small allocations).",
      "fix_suggestion": "** Either (a) increment the entry's refcount before dropping the lock, or (b) copy the name/value data into a local buffer before dropping the lock, or (c) use `GFP_ATOMIC` allocation under the lock (acceptable for small allocations).",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-060",
      "title": "HTTP/3 Stream Lookup: Use-After-Free (No Refcount)",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Remote attacker can trigger this by rapidly opening and closing streams while sending frames that reference those streams. Results in use-after-free, potential code execution.\n- **Recommendation:** Add `refcount_inc(&stream->refcount)` in `h3_stream_lookup()` and require all callers to call a corresponding `h3_stream_put()` when done.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_stream.c"
        ],
        "symbols": [
          "Free",
          "h3_stream_lookup",
          "h3_stream_put",
          "refcount_inc"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `h3_stream_lookup()` returns a raw pointer to an `h3_stream` without incrementing its reference count. Callers use this pointer outside the lock that protected the lookup. If another thread closes/destroys the stream concurrently, the caller dereferences freed memory.\n- **Impact:** Remote attacker can trigger this by rapidly opening and closing streams while sending frames that reference those streams. Results in use-after-free, potential code execution.\n- **Recommendation:** Add `refcount_inc(&stream->refcount)` in `h3_stream_lookup()` and require all callers to call a corresponding `h3_stream_put()` when done.",
      "fix_suggestion": "** Add `refcount_inc(&stream->refcount)` in `h3_stream_lookup()` and require all callers to call a corresponding `h3_stream_put()` when done.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-061",
      "title": "Connection Destroy Calls Sleeping Function Under Spinlock",
      "category": "concurrency",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Kernel panic (BUG) during connection teardown, causing denial of service. Easily triggered by any connection close.\n- **Recommendation:** Collect stream pointers into a local list under the spinlock, release the spinlock, then close each stream outside the lock.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_stream.c"
        ],
        "symbols": [
          "h3_connection_destroy",
          "panic",
          "tquic_stream_close"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `h3_connection_destroy()` iterates over streams under a spinlock and calls `tquic_stream_close()` for each. `tquic_stream_close()` may invoke allocation paths with `GFP_KERNEL` (sleeping allocation) or other sleeping operations. Calling a potentially-sleeping function under a spinlock causes a BUG on kernels with `CONFIG_DEBUG_ATOMIC_SLEEP`.\n- **Impact:** Kernel panic (BUG) during connection teardown, causing denial of service. Easily triggered by any connection close.\n- **Recommendation:** Collect stream pointers into a local list under the spinlock, release the spinlock, then close each stream outside the lock.",
      "fix_suggestion": "** Collect stream pointers into a local list under the spinlock, release the spinlock, then close each stream outside the lock.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-062",
      "title": "Path Metrics Netlink: Unbounded Allocation from Attacker-Influenced Value",
      "category": "security",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Local denial of service via repeated large kernel allocations. If `num_paths` validation is ever bypassed, attacker-controlled allocation size.\n- **Recommendation:** Cap the allocation at a fixed reasonable maximum (e.g., `min(conn->num_paths, TQUIC_MAX_PATHS) * NLMSG_DEFAULT_SIZE`), and add the CAP_NET_ADMIN check from H1.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/diag/path_metrics.c"
        ],
        "symbols": [
          "allocation",
          "connection",
          "maximum",
          "min",
          "nlmsg_new",
          "tquic_nl_get_all_paths"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The function allocates `nlmsg_new(NLMSG_DEFAULT_SIZE * conn->num_paths, GFP_KERNEL)`. `conn->num_paths` is influenced by the number of paths on a connection (up to `TQUIC_MAX_PATHS` = 16, but the multiplication with `NLMSG_DEFAULT_SIZE` (typically 8192) can be up to 128KB). While `TQUIC_MAX_PATHS=16` provides some bound, if `num_paths` is ever corrupted or the limit is raised, this becomes an unbounded allocation. More importantly, any unprivileged user can trigger this allocation (see H1).\n- **Impact:** Local denial of service via repeated large kernel allocations. If `num_paths` validation is ever bypassed, attacker-controlled allocation size.\n- **Recommendation:** Cap the allocation at a fixed reasonable maximum (e.g., `min(conn->num_paths, TQUIC_MAX_PATHS) * NLMSG_DEFAULT_SIZE`), and add the CAP_NET_ADMIN check from H1.",
      "fix_suggestion": "** Cap the allocation at a fixed reasonable maximum (e.g., `min(conn->num_paths, TQUIC_MAX_PATHS) * NLMSG_DEFAULT_SIZE`), and add the CAP_NET_ADMIN check from H1.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-063",
      "title": "Path Metrics Netlink: Missing CAP_NET_ADMIN Permission Check",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Information disclosure of network connection metadata (RTT, bandwidth, loss rates, connection IDs, peer addresses) to unprivileged users. Subscription flooding can also cause kernel memory exhaustion.\n- **Recommendation:** Add `.policy` with `GENL_ADMIN_PERM` flag or explicit `CAP_NET_ADMIN` check in each handler.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/diag/path_metrics.c"
        ],
        "symbols": [
          "export",
          "metadata"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The genetlink operations for path metrics export (`tquic_nl_get_path_metrics`, `tquic_nl_get_all_paths`, `tquic_nl_subscribe_events`) do not require `CAP_NET_ADMIN`. Any unprivileged local user can query connection metrics, subscribe to events, and enumerate all TQUIC connections.\n- **Impact:** Information disclosure of network connection metadata (RTT, bandwidth, loss rates, connection IDs, peer addresses) to unprivileged users. Subscription flooding can also cause kernel memory exhaustion.\n- **Recommendation:** Add `.policy` with `GENL_ADMIN_PERM` flag or explicit `CAP_NET_ADMIN` check in each handler.",
      "fix_suggestion": "** Add `.policy` with `GENL_ADMIN_PERM` flag or explicit `CAP_NET_ADMIN` check in each handler.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-064",
      "title": "QPACK Decoder: Unbounded Blocked Stream Memory Exhaustion",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Remote attacker can exhaust kernel memory by sending many large header blocks that all reference high insert counts. With `max_blocked_streams=100` and headers up to the frame size limit, this could consume hundreds of megabytes.\n- **Recommendation:** Track total blocked stream memory and enforce a per-connection limit (e.g., 1MB total blocked stream data).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/qpack_decoder.c"
        ],
        "symbols": [
          "blocks",
          "kmemdup",
          "limit",
          "received"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** When a header block references a dynamic table entry that has not yet been received (insert count > known received count), the decoder stores the entire header block data via `kmemdup(data, len, GFP_ATOMIC)`. While there is a `max_blocked_streams` limit on the count of blocked streams, there is no limit on the total memory consumed by blocked stream data. An attacker can send large header blocks (up to the maximum allowed) on each blocked stream.\n- **Impact:** Remote attacker can exhaust kernel memory by sending many large header blocks that all reference high insert counts. With `max_blocked_streams=100` and headers up to the frame size limit, this could consume hundreds of megabytes.\n- **Recommendation:** Track total blocked stream memory and enforce a per-connection limit (e.g., 1MB total blocked stream data).",
      "fix_suggestion": "** Track total blocked stream memory and enforce a per-connection limit (e.g., 1MB total blocked stream data).",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-065",
      "title": "WebTransport Context Destroy: Lock Drop During Iteration",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Kernel memory corruption during WebTransport session teardown, triggerable by a remote peer closing sessions concurrently.\n- **Recommendation:** Use a safe iteration pattern: move items to a local list under the lock, release the lock, then process the local list.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/webtransport.c"
        ],
        "symbols": [
          "context_destroy"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** During WebTransport context destruction, the function iterates over sessions and drops/re-acquires the lock for cleanup operations. This creates a window where the session list can be modified by concurrent operations, potentially causing list corruption or use-after-free of the iteration cursor.\n- **Impact:** Kernel memory corruption during WebTransport session teardown, triggerable by a remote peer closing sessions concurrently.\n- **Recommendation:** Use a safe iteration pattern: move items to a local list under the lock, release the lock, then process the local list.",
      "fix_suggestion": "** Use a safe iteration pattern: move items to a local list under the lock, release the lock, then process the local list.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-066",
      "title": "WebTransport: Unbounded Capsule Buffer Growth",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Remote denial of service via kernel memory exhaustion.\n- **Recommendation:** Enforce a maximum capsule buffer size (e.g., 64KB or configurable via socket option) and reject connections that exceed it with `H3_EXCESSIVE_LOAD`.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/webtransport.c"
        ],
        "symbols": [
          "capsule",
          "size"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The capsule parsing code accumulates incoming capsule data into `capsule_buf` without enforcing a maximum total size. A remote peer can send an arbitrarily large capsule (or many partial capsules) to exhaust kernel memory.\n- **Impact:** Remote denial of service via kernel memory exhaustion.\n- **Recommendation:** Enforce a maximum capsule buffer size (e.g., 64KB or configurable via socket option) and reject connections that exceed it with `H3_EXCESSIVE_LOAD`.",
      "fix_suggestion": "** Enforce a maximum capsule buffer size (e.g., 64KB or configurable via socket option) and reject connections that exceed it with `H3_EXCESSIVE_LOAD`.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-067",
      "title": "HTTP/3 Settings Frame Length Truncation",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Protocol confusion: the truncated length causes the peer to misparse the stream, potentially leading to security-relevant misinterpretation of subsequent frames. While 255 bytes is sufficient for typical settings, extensions or future settings could exceed this.\n- **Recommendation:** Use proper QUIC variable-length integer encoding for the frame length, or validate that `settings_len <= 255` before the cast and return an error if exceeded.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_stream.c"
        ],
        "symbols": [
          "h3_connection_send_settings"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The settings frame length is cast to `u8`: `*len_pos = (u8)settings_len;`. If the total settings payload exceeds 255 bytes, the length field is silently truncated. The peer will parse an incomplete settings frame, potentially interpreting subsequent data as a new frame.\n- **Impact:** Protocol confusion: the truncated length causes the peer to misparse the stream, potentially leading to security-relevant misinterpretation of subsequent frames. While 255 bytes is sufficient for typical settings, extensions or future settings could exceed this.\n- **Recommendation:** Use proper QUIC variable-length integer encoding for the frame length, or validate that `settings_len <= 255` before the cast and return an error if exceeded.",
      "fix_suggestion": "** Use proper QUIC variable-length integer encoding for the frame length, or validate that `settings_len <= 255` before the cast and return an error if exceeded.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-068",
      "title": "QPACK Encoder: Insert Count Increment Overflow",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** The encoder may reference non-existent dynamic table entries, causing the decoder to fail or reference wrong entries. This could lead to header injection if the wrong entry is referenced.\n- **Recommendation:** Validate that `known_received_count + value <= insert_count` (the total entries ever inserted) and that the addition does not overflow.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/qpack_encoder.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The Insert Count Increment instruction handler adds the received value to `known_received_count` without overflow checking: `known_received_count += value;`. An attacker can send a crafted increment value that wraps the counter, causing the encoder to believe the decoder has received entries that do not exist.\n- **Impact:** The encoder may reference non-existent dynamic table entries, causing the decoder to fail or reference wrong entries. This could lead to header injection if the wrong entry is referenced.\n- **Recommendation:** Validate that `known_received_count + value <= insert_count` (the total entries ever inserted) and that the addition does not overflow.",
      "fix_suggestion": "** Validate that `known_received_count + value <= insert_count` (the total entries ever inserted) and that the addition does not overflow.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-069",
      "title": "HTTP/3 Request: TOCTOU Between State Check and Send",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Protocol violation, potential data corruption or crash if send operates on a closed/reset stream.\n- **Recommendation:** Either hold the lock during the entire send operation, or re-validate state after acquiring any needed resources.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_request.c"
        ],
        "symbols": [
          "h3_request_send_headers",
          "state"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The function checks the stream state under a lock, releases the lock, then performs the actual send operation without the lock. Between the check and the send, another thread can change the stream state (e.g., close the stream), causing the send to operate on a stream in an invalid state.\n- **Impact:** Protocol violation, potential data corruption or crash if send operates on a closed/reset stream.\n- **Recommendation:** Either hold the lock during the entire send operation, or re-validate state after acquiring any needed resources.",
      "fix_suggestion": "** Either hold the lock during the entire send operation, or re-validate state after acquiring any needed resources.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-070",
      "title": "HTTP/3 Frame Parsing: 16MB Maximum Frame Payload",
      "category": "security",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** A remote attacker can trigger large kernel allocations by sending frames with large payload lengths.\n- **Recommendation:** Consider reducing the default limit to 1MB or making it configurable. Most HTTP/3 frames (HEADERS, SETTINGS, GOAWAY) should be much smaller.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_frame.c"
        ],
        "symbols": [
          "frames"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `H3_MAX_FRAME_PAYLOAD_SIZE` is set to 16MB. While this is a valid limit, it means a single frame can cause a 16MB kernel allocation. Under memory pressure, this is significant.\n- **Impact:** A remote attacker can trigger large kernel allocations by sending frames with large payload lengths.\n- **Recommendation:** Consider reducing the default limit to 1MB or making it configurable. Most HTTP/3 frames (HEADERS, SETTINGS, GOAWAY) should be much smaller.",
      "fix_suggestion": "** Consider reducing the default limit to 1MB or making it configurable. Most HTTP/3 frames (HEADERS, SETTINGS, GOAWAY) should be much smaller.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-071",
      "title": "QPACK Huffman Decoder: O(n*256) Complexity",
      "category": "security",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** CPU exhaustion: an attacker can send Huffman-encoded headers that maximize decoding time. With large headers, this becomes a practical slowloris-style attack.\n- **Recommendation:** Replace with a 256-entry lookup table or state-machine-based decoder (standard approach for HPACK/QPACK Huffman).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/qpack.c"
        ],
        "symbols": [
          "decoder"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The Huffman decoder uses a brute-force O(n * 256) algorithm that iterates over all 256 possible symbols for each input byte. This is significantly slower than a proper lookup-table or tree-based decoder.\n- **Impact:** CPU exhaustion: an attacker can send Huffman-encoded headers that maximize decoding time. With large headers, this becomes a practical slowloris-style attack.\n- **Recommendation:** Replace with a 256-entry lookup table or state-machine-based decoder (standard approach for HPACK/QPACK Huffman).",
      "fix_suggestion": "** Replace with a 256-entry lookup table or state-machine-based decoder (standard approach for HPACK/QPACK Huffman).",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-072",
      "title": "QPACK Integer Decode: Shift Overflow",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Undefined behavior (implementation-defined on most architectures) that could produce incorrect values, leading to buffer overflows or other memory safety issues.\n- **Recommendation:** Add a check: `if (shift > 62) return -H3_ERR_QPACK_DECOMPRESSION_FAILED;`",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/qpack.c"
        ],
        "symbols": [
          "behavior"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The QPACK integer decode loop accumulates `value |= (byte & 0x7f) << shift` and increments `shift` by 7 each iteration. If the input contains many continuation bytes, `shift` can exceed 63, causing undefined behavior for the left shift on a `u64`.\n- **Impact:** Undefined behavior (implementation-defined on most architectures) that could produce incorrect values, leading to buffer overflows or other memory safety issues.\n- **Recommendation:** Add a check: `if (shift > 62) return -H3_ERR_QPACK_DECOMPRESSION_FAILED;`",
      "fix_suggestion": "** Add a check: `if (shift > 62) return -H3_ERR_QPACK_DECOMPRESSION_FAILED;`",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-073",
      "title": "QPACK Encoder/Decoder: Excessive Stack Usage",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Stack overflow if these functions are called from a deep call chain (e.g., interrupt context -> softirq -> QUIC receive -> QPACK decode). Results in kernel panic.\n- **Recommendation:** Allocate these buffers dynamically with `kmalloc(GFP_ATOMIC)` or use a pre-allocated per-connection buffer.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/qpack_encoder.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/qpack_encoder.c`, `qpack_decoder.c"
        ],
        "symbols": [
          "chain",
          "qpack_encoder_insert_literal",
          "qpack_encoder_insert_name_ref"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Multiple functions allocate large buffers on the stack:\n  - `qpack_decoder`: `name_buf[256]` + `value_buf[8192]` = 8448 bytes\n  - `qpack_encoder_insert_name_ref()`: `buf[QPACK_MAX_HEADER_VALUE_LEN + 32]` = 8224 bytes\n  - `qpack_encoder_insert_literal()`: `buf[8512]` = 8512 bytes",
      "fix_suggestion": "** Allocate these buffers dynamically with `kmalloc(GFP_ATOMIC)` or use a pre-allocated per-connection buffer.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-074",
      "title": "HTTP/3 Settings Parser: TOCTOU on Settings Count",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** If the buffer is shared, settings could be double-processed or skipped, potentially leading to protocol confusion.\n- **Recommendation:** Ensure the input buffer is exclusively owned during parsing, or perform length validation inline during the single-pass parse.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_frame.c"
        ],
        "symbols": [
          "processing"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The settings parser checks the count/length of settings, then processes them in a separate pass. If the underlying data can change between the check and the processing (e.g., if the buffer is shared), this is a TOCTOU vulnerability. In practice, this depends on whether the input buffer is guaranteed to be stable.\n- **Impact:** If the buffer is shared, settings could be double-processed or skipped, potentially leading to protocol confusion.\n- **Recommendation:** Ensure the input buffer is exclusively owned during parsing, or perform length validation inline during the single-pass parse.",
      "fix_suggestion": "** Ensure the input buffer is exclusively owned during parsing, or perform length validation inline during the single-pass parse.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-075",
      "title": "HTTP/3 Connection: O(n) Push Entry Counting",
      "category": "security",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** CPU exhaustion for servers processing many push promises from a malicious client (or vice versa).\n- **Recommendation:** Maintain a running counter of push entries instead of counting on demand.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_conn.c"
        ],
        "symbols": [
          "client"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The connection counts push entries by iterating the entire push list on each operation, resulting in O(n) complexity. An attacker can create many push promises to make subsequent operations slow.\n- **Impact:** CPU exhaustion for servers processing many push promises from a malicious client (or vice versa).\n- **Recommendation:** Maintain a running counter of push entries instead of counting on demand.",
      "fix_suggestion": "** Maintain a running counter of push entries instead of counting on demand.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-076",
      "title": "WebTransport: TOCTOU in Datagram Queue Push",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Queue size limit bypass, potentially leading to memory exhaustion if the limit is the only defense against unbounded growth.\n- **Recommendation:** Perform the size check and push atomically under a lock, or use an atomic counter with compare-and-swap.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/webtransport.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The datagram queue checks its current size, then pushes a new element. If two threads push concurrently, both may see the queue as under-limit and both push, exceeding the intended limit.\n- **Impact:** Queue size limit bypass, potentially leading to memory exhaustion if the limit is the only defense against unbounded growth.\n- **Recommendation:** Perform the size check and push atomically under a lock, or use an atomic counter with compare-and-swap.",
      "fix_suggestion": "** Perform the size check and push atomically under a lock, or use an atomic counter with compare-and-swap.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-077",
      "title": "Qlog Ring Buffer: Not Truly Lock-Free",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Performance degradation under high qlog event rates. Not a security vulnerability per se, but the misleading documentation could lead to incorrect assumptions about safety in interrupt context.\n- **Recommendation:** Either implement a true lock-free ring buffer (using `smp_store_release`/`smp_load_acquire` pairs) or document the locking requirement clearly.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/diag/qlog.c"
        ],
        "symbols": [
          "buffer"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The qlog ring buffer is documented as lock-free but actually uses a spinlock for write operations. Under high event rates, this creates contention on the hot path.\n- **Impact:** Performance degradation under high qlog event rates. Not a security vulnerability per se, but the misleading documentation could lead to incorrect assumptions about safety in interrupt context.\n- **Recommendation:** Either implement a true lock-free ring buffer (using `smp_store_release`/`smp_load_acquire` pairs) or document the locking requirement clearly.",
      "fix_suggestion": "** Either implement a true lock-free ring buffer (using `smp_store_release`/`smp_load_acquire` pairs) or document the locking requirement clearly.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-078",
      "title": "Qlog: JSON Strings Not Escaped",
      "category": "api",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Malformed JSON output that could cause parsing failures in qlog consumers. If a qlog consumer naively processes the JSON (e.g., injecting into a web dashboard), this could be an XSS vector.\n- **Recommendation:** Implement JSON string escaping for all string values emitted in qlog JSON output.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/diag/qlog.c"
        ],
        "symbols": [
          "JSON",
          "values"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** When emitting JSON-formatted qlog events, string values (such as reason phrases from CONNECTION_CLOSE frames) are included without JSON escaping. If a reason phrase contains characters like `\"`, `\\`, or control characters, the resulting JSON is malformed.\n- **Impact:** Malformed JSON output that could cause parsing failures in qlog consumers. If a qlog consumer naively processes the JSON (e.g., injecting into a web dashboard), this could be an XSS vector.\n- **Recommendation:** Implement JSON string escaping for all string values emitted in qlog JSON output.",
      "fix_suggestion": "** Implement JSON string escaping for all string values emitted in qlog JSON output.",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-079",
      "title": "Path Metrics Subscription: Timer/Connection Lifetime Race",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Use-after-free when the timer fires on a freed connection, causing kernel memory corruption.\n- **Recommendation:** Use `del_timer_sync()` in `tquic_metrics_unsubscribe_conn()` to ensure the timer callback has completed before freeing the connection, or hold a connection reference in the subscription.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/diag/path_metrics.c"
        ],
        "symbols": [
          "del_timer_sync",
          "tquic_metrics_unsubscribe_conn"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** A metrics subscription timer can fire after the associated connection has been freed, if `tquic_metrics_unsubscribe_conn()` races with the timer callback. The timer callback accesses the connection pointer without verifying it is still valid.\n- **Impact:** Use-after-free when the timer fires on a freed connection, causing kernel memory corruption.\n- **Recommendation:** Use `del_timer_sync()` in `tquic_metrics_unsubscribe_conn()` to ensure the timer callback has completed before freeing the connection, or hold a connection reference in the subscription.",
      "fix_suggestion": "** Use `del_timer_sync()` in `tquic_metrics_unsubscribe_conn()` to ensure the timer callback has completed before freeing the connection, or hold a connection reference in the subscription.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-080",
      "title": "Duplicate Static Functions: h3_varint_encode/decode",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Maintenance risk; potential for divergent behavior if one copy is patched but not the other.\n- **Recommendation:** Move to a shared helper (e.g., in `http3_frame.c` or a common header as inline functions).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_request.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_request.c`, `http3_stream.c"
        ],
        "symbols": [
          "h3_varint_decode",
          "h3_varint_encode",
          "helper"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Both files contain independent static implementations of varint encode/decode functions. If one is fixed for a bug and the other is not, inconsistent behavior results.\n- **Impact:** Maintenance risk; potential for divergent behavior if one copy is patched but not the other.\n- **Recommendation:** Move to a shared helper (e.g., in `http3_frame.c` or a common header as inline functions).",
      "fix_suggestion": "** Move to a shared helper (e.g., in `http3_frame.c` or a common header as inline functions).",
      "tests_to_add": [],
      "notes": "Line evidence is approximate/non-normalized in source text."
    },
    {
      "id": "F-081",
      "title": "HTTP/3 Priority: push_buckets Not Initialized",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": "** If the allocation path changes to use `kmalloc` instead of `kzalloc`, the array would contain garbage, leading to incorrect scheduling or null pointer dereferences.\n- **Recommendation:** Explicitly initialize `push_buckets[]` in the priority init function for defensive coding.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/http3/http3_priority.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The `push_buckets[]` array used for server push scheduling is not explicitly initialized in the priority scheduler initialization path. It relies on the structure being zeroed by `kzalloc`, which is correct but fragile.\n- **Impact:** If the allocation path changes to use `kmalloc` instead of `kzalloc`, the array would contain garbage, leading to incorrect scheduling or null pointer dereferences.\n- **Recommendation:** Explicitly initialize `push_buckets[]` in the priority init function for defensive coding.",
      "fix_suggestion": "** Explicitly initialize `push_buckets[]` in the priority init function for defensive coding.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-082",
      "title": "Qlog: Lock Drop Around copy_to_user",
      "category": "concurrency",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Userspace may receive a partially old/partially new event if the ring buffer wraps during the copy. This is a data integrity issue, not a security vulnerability.\n- **Recommendation:** Copy the entry to a local kernel buffer under the lock, then `copy_to_user` from the local buffer.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/diag/qlog.c"
        ],
        "symbols": [
          "behavior",
          "copy_to_user"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The qlog read path drops the ring buffer lock before calling `copy_to_user()` (which may sleep). This is correct behavior (you cannot hold a spinlock across `copy_to_user`), but the entry being copied could be overwritten by a new event between the lock drop and the copy completion.\n- **Impact:** Userspace may receive a partially old/partially new event if the ring buffer wraps during the copy. This is a data integrity issue, not a security vulnerability.\n- **Recommendation:** Copy the entry to a local kernel buffer under the lock, then `copy_to_user` from the local buffer.",
      "fix_suggestion": "** Copy the entry to a local kernel buffer under the lock, then `copy_to_user` from the local buffer.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-083",
      "title": "Benchmark Code: Userspace, Not Kernel",
      "category": "memory",
      "severity": "S3",
      "confidence": "low",
      "impact": "** No kernel security impact. The code is a test harness only.\n- **Recommendation:** Consider moving benchmark code to a `tools/` or `tests/` directory to avoid confusion about its execution context.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/bench/bench_common.c"
        ],
        "symbols": [
          "APIs",
          "Diagnostics",
          "Immediate",
          "QPACK",
          "functions",
          "handling",
          "limits",
          "min_t",
          "patterns",
          "priority",
          "qpack_static_get",
          "quic_trace_conn_id"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The benchmark code in `net/tquic/bench/` is userspace code using libc functions (`printf`, `malloc`, `FILE *`, `math.h`). It does not run in kernel context and is not compiled as part of the kernel module.\n- **Impact:** No kernel security impact. The code is a test harness only.\n- **Recommendation:** Consider moving benchmark code to a `tools/` or `tests/` directory to avoid confusion about its execution context.",
      "fix_suggestion": "** Consider moving benchmark code to a `tools/` or `tests/` directory to avoid confusion about its execution context.",
      "tests_to_add": [
        "**Impact:** No kernel security impact. The code is a test harness only.",
        "**Recommendation:** Consider moving benchmark code to a `tools/` or `tests/` directory to avoid confusion about its execution context."
      ],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-084",
      "title": "CRITICAL: Busy-poll per-packet lock/unlock",
      "category": "concurrency",
      "severity": "S0",
      "confidence": "high",
      "impact": "** In `tquic_busy_poll()`, the spinlock is acquired and released for every single packet. At high packet rates (100k+ pps), this creates extreme lock contention and cache-line bouncing. By contrast, the NAPI poll path at line 317 correctly uses batch dequeue via `skb_queue_splice_init()`.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:460-465"
        ],
        "symbols": [
          "__skb_dequeue",
          "atomic_dec",
          "rates",
          "skb_queue_splice_init",
          "spin_lock_irqsave",
          "spin_unlock_irqrestore",
          "tquic_busy_poll",
          "tquic_napi_poll"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:460-465"
        ],
        "snippets": [
          "while (work_done < budget) {\n    spin_lock_irqsave(&tn->lock, flags);\n    skb = __skb_dequeue(&tn->rx_queue);\n    if (skb)\n        atomic_dec(&tn->rx_queue_len);\n    spin_unlock_irqrestore(&tn->lock, flags);\n    ...\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use the same batch-splice pattern as `tquic_napi_poll()`: splice the queue to a local list under a single lock acquisition, then process without holding the lock.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-085",
      "title": "HIGH: conn->lock held during path selection on every TX packet",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Every TX packet acquires `conn->lock` with BH disabled just to read the active path pointer. For single-path connections (common case), this is unnecessary contention. The scheduler path may do significant work under the lock.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1121-1132"
        ],
        "symbols": [
          "READ_ONCE",
          "connections",
          "spin_lock_bh",
          "spin_unlock_bh",
          "tquic_bond_select_path",
          "tquic_select_path"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1121-1132"
        ],
        "snippets": [
          "struct tquic_path *tquic_select_path(...)\n{\n    spin_lock_bh(&conn->lock);\n    if (conn->scheduler)\n        selected = tquic_bond_select_path(conn, skb);\n    else\n        selected = conn->active_path;\n    spin_unlock_bh(&conn->lock);\n    return selected;\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** For the single-path fast path, use `READ_ONCE(conn->active_path)` without the lock. Only take the lock when a scheduler is configured. Consider RCU protection for the path list.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-086",
      "title": "HIGH: conn->paths_lock in RX path for every packet",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Linear search through path list under spinlock on every received packet. For connections with many paths, this is O(n) under lock.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:251-258"
        ],
        "symbols": [
          "first",
          "list_for_each_entry",
          "spin_lock_bh",
          "table",
          "tquic_find_path_by_addr"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:251-258"
        ],
        "snippets": [
          "static struct tquic_path *tquic_find_path_by_addr(...)\n{\n    spin_lock_bh(&conn->paths_lock);\n    list_for_each_entry(path, &conn->paths, list) {\n        if (memcmp(&path->remote_addr, addr, sizeof(*addr)) == 0) {\n            ..."
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use a hash table (rhashtable) for path-by-address lookup. For single-path connections, cache the last-used path and check it first (fast-path optimization).",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-087",
      "title": "MEDIUM: conn->streams_lock for RB-tree walk on every STREAM frame",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** The RB-tree lookup is O(log n) but holds BH-disabled spinlock. If many streams are active, this can cause lock contention with concurrent TX path stream operations.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:917-934"
        ],
        "symbols": [
          "spin_lock_bh",
          "spin_unlock_bh"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:917-934"
        ],
        "snippets": [
          "spin_lock_bh(&ctx->conn->streams_lock);\n{\n    struct rb_node *node = ctx->conn->streams.rb_node;\n    while (node) { ... }\n}\nspin_unlock_bh(&ctx->conn->streams_lock);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Consider RCU-protected RB-tree for read-side lookups, or use a lockless hash table for the common case of looking up an already-existing stream.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-088",
      "title": "MEDIUM: FEC encoder double lock nesting",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Nested locks on the encoding path. The inner `block->lock` is taken while `enc->lock` is already held. This is safe (consistent ordering) but adds overhead. Since the outer lock already serializes access, the inner lock may be unnecessary.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/fec/fec_encoder.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/fec/fec_encoder.c:275-283"
        ],
        "symbols": [
          "list_add_tail",
          "safe",
          "spin_lock_bh",
          "spin_unlock_bh"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/fec/fec_encoder.c:275-283"
        ],
        "snippets": [
          "spin_lock_bh(&enc->lock);\n...\n    spin_lock(&block->lock);\n    list_add_tail(&symbol->list, &block->source_symbols);\n    ...\n    spin_unlock(&block->lock);\n...\nspin_unlock_bh(&enc->lock);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Remove `block->lock` when the block is only accessed under `enc->lock`, or redesign to avoid nesting.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-089",
      "title": "MEDIUM: GRO flush drops and reacquires lock in loop",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Lock ping-pong for every packet during GRO flush. The deliver callback may be expensive.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2303-2310"
        ],
        "symbols": [
          "__skb_dequeue",
          "deliver"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2303-2310"
        ],
        "snippets": [
          "while ((skb = __skb_dequeue(&gro->hold_queue)) != NULL) {\n    spin_unlock(&gro->lock);\n    deliver(skb);\n    flushed++;\n    spin_lock(&gro->lock);\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Splice the queue under a single lock hold, then deliver all packets without the lock.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-090",
      "title": "CRITICAL: Per-packet kmalloc for frame data in TX path",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Two GFP_ATOMIC allocations per packet in `tquic_xmit()`. GFP_ATOMIC is expensive (cannot sleep, must use reserve pools). The frame struct is then freed in `tquic_coalesce_frames()` after being used once. This is a classic allocation-per-packet anti-pattern.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1806-1826"
        ],
        "symbols": [
          "cache",
          "expensive",
          "memcpy",
          "structs",
          "tquic_assemble_packet",
          "tquic_coalesce_frames",
          "tquic_gen_stream_frame",
          "tquic_xmit"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1806-1826"
        ],
        "snippets": [
          "frame = kzalloc(sizeof(*frame), GFP_ATOMIC);  /* struct allocation */\n...\nframe->data = kmalloc(chunk, GFP_ATOMIC);     /* data copy */\nmemcpy(frame->data, data + offset, chunk);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "**\n1. Use a slab cache (`kmem_cache`) for `tquic_pending_frame` structs (fixed size, high churn).\n2. Eliminate the intermediate data copy entirely -- write STREAM frame data directly into the skb payload buffer during `tquic_assemble_packet()`. The current architecture allocates frame->data, copies data in, then copies again into the skb via `memcpy()` in `tquic_gen_stream_frame()`. This is a double copy.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-091",
      "title": "HIGH: Per-STREAM-frame skb allocation in RX path",
      "category": "perf",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Every received STREAM frame allocates a new skb and copies data into it. For high-throughput streams, this is significant overhead.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:944-948"
        ],
        "symbols": [
          "alloc_skb",
          "directly",
          "skb_put_data"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:944-948"
        ],
        "snippets": [
          "data_skb = alloc_skb(length, GFP_ATOMIC);\nif (!data_skb)\n    return -ENOMEM;\nskb_put_data(data_skb, ctx->data + ctx->offset, length);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Consider using page fragments or referencing the original decrypted buffer directly (with proper lifetime management) instead of copying per-frame.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-092",
      "title": "HIGH: CONNECTION_CLOSE uses kmalloc for small buffer",
      "category": "perf",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Small fixed-size buffer allocated on heap in atomic context. This could be a stack allocation.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1950-1951"
        ],
        "symbols": [
          "does",
          "tquic_send_ack"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1950-1951"
        ],
        "snippets": [
          "buf = kmalloc(256, GFP_ATOMIC);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use a stack buffer like `tquic_send_ack()` does (line 1877: `u8 buf_stack[128]`). A 256-byte stack allocation is safe in kernel context.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-093",
      "title": "MEDIUM: Slab cache for RX decryption is good practice",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Positive finding -- the RX path correctly uses a slab cache for the common MTU-sized case. The fallback to kmalloc for jumbo packets is appropriate.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2585-2590"
        ],
        "symbols": [
          "kmem_cache_alloc",
          "likely"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2585-2590"
        ],
        "snippets": [
          "if (likely(payload_len <= TQUIC_RX_BUF_SIZE)) {\n    decrypted = kmem_cache_alloc(tquic_rx_buf_cache, GFP_ATOMIC);\n    decrypted_from_slab = true;\n} else {\n    decrypted = kmalloc(payload_len, GFP_ATOMIC);\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-094",
      "title": "MEDIUM: io_uring async data uses kzalloc per request",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Each io_uring async operation allocates 64+ bytes via kzalloc. io_uring's design intent is to avoid per-request allocations by using the request's embedded data area.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:139"
        ],
        "symbols": [
          "area",
          "io_alloc_async_data",
          "io_kiocb_to_cmd"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:139"
        ],
        "snippets": [
          "data = kzalloc(sizeof(*data), GFP_KERNEL);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use `io_alloc_async_data()` or embed the async data in the `io_kiocb` command data area (the `io_kiocb_to_cmd()` pattern already used for send/recv).",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-095",
      "title": "CRITICAL: Double data copy in TX path",
      "category": "correctness",
      "severity": "S0",
      "confidence": "high",
      "impact": "** For 1200-byte payloads at line rate, this is ~2.4 GB/s of unnecessary memcpy at 10 Gbps.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1819-1825"
        ],
        "symbols": [
          "tquic_gen_stream_frame",
          "tquic_xmit"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1819-1825",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1819-1825` then `365"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Restructure `tquic_xmit()` to write directly into the skb's linear data area, eliminating the intermediate `tquic_pending_frame` allocation and copy.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-096",
      "title": "HIGH: Full sockaddr_storage memcmp for path lookup",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** `sizeof(struct sockaddr_storage)` is 128 bytes. Comparing 128 bytes per path for every packet is expensive. Most of those bytes are padding.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:253"
        ],
        "symbols": [],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:253"
        ],
        "snippets": [
          "if (memcmp(&path->remote_addr, addr, sizeof(*addr)) == 0)"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Compare only the relevant fields: for IPv4 compare `sin_family + sin_addr + sin_port` (8 bytes); for IPv6 compare `sin6_family + sin6_addr + sin6_port` (22 bytes). Or pre-compute and store a hash for fast comparison.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-097",
      "title": "MEDIUM: Zerocopy sendmsg chunks at 1200 bytes",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** When SG is supported, the zerocopy path processes data in 1200-byte chunks. Each chunk allocates an skb. For large sends (e.g., 64KB), this creates ~54 skbs.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:293"
        ],
        "symbols": [
          "chunks",
          "min_t",
          "sends"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:293"
        ],
        "snippets": [
          "size_t chunk = min_t(size_t, len - copied, 1200);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use larger chunks (up to GSO segment size) and coalesce page fragments into fewer skbs.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-098",
      "title": "HIGH: Dual global atomic + per-CPU stats in NAPI poll",
      "category": "api",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Every NAPI poll and every packet processed updates both per-CPU counters AND global atomic counters. The global atomics cause cache-line bouncing across CPUs. The per-CPU counters are already sufficient -- the global counters are redundant.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:307-308"
        ],
        "symbols": [
          "atomic64_add",
          "atomic64_inc",
          "this_cpu_add",
          "this_cpu_inc",
          "tquic_napi_aggregate_pcpu_stats"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:307-308"
        ],
        "snippets": [
          "this_cpu_inc(tquic_napi_pcpu_stats.total_polls);\natomic64_inc(&tquic_napi_global_stats.total_polls);",
          "this_cpu_add(tquic_napi_pcpu_stats.total_packets, work_done);\natomic64_add(work_done, &tquic_napi_global_stats.total_packets);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Remove the `tquic_napi_global_stats` atomic counters entirely. Use `tquic_napi_aggregate_pcpu_stats()` (already implemented at line 75) when global totals are needed for /proc display.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-099",
      "title": "HIGH: GRO stats use global atomic64 on every packet",
      "category": "correctness",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Every coalesced packet touches a global atomic counter, causing cross-CPU cache-line invalidation in the GRO path.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c:295"
        ],
        "symbols": [
          "atomic64_inc"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c:295"
        ],
        "snippets": [
          "atomic64_inc(&tquic_gro_stats.coalesced_packets);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use per-CPU counters for GRO statistics, aggregate on read.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-100",
      "title": "MEDIUM: MIB counter updates on every packet in RX/TX paths",
      "category": "perf",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** The `TQUIC_INC_STATS` macro (likely per-CPU) is fine, but there are two calls per packet (one for count, one for bytes). Consider batching or combining.",
      "evidence": {
        "file_paths": [],
        "symbols": [
          "TQUIC_ADD_STATS",
          "TQUIC_INC_STATS",
          "ktime_get",
          "macro",
          "packet",
          "sock_net"
        ],
        "line_ranges": [],
        "snippets": [
          "TQUIC_INC_STATS(sock_net(conn->sk), TQUIC_MIB_PACKETSRX);\nTQUIC_ADD_STATS(sock_net(conn->sk), TQUIC_MIB_BYTESRX, len);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Acceptable overhead if using per-CPU counters. Verify the macro implementation uses `this_cpu_add` rather than atomics.",
      "tests_to_add": [],
      "notes": "Missing file path evidence in source text. Missing line-range evidence in source text."
    },
    {
      "id": "F-101",
      "title": "HIGH: Multiple ktime_get() calls per packet",
      "category": "perf",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** `ktime_get()` is relatively cheap on modern x86 (RDTSC-based, ~20ns) but adds up at high packet rates. Multiple calls per packet sum to ~100ns/pkt.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c"
        ],
        "symbols": [
          "ktime_get",
          "x86"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:305",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:2687",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:713",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1736"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Read the timestamp once at the start of packet processing and pass it through the context. The `tquic_rx_ctx` struct already exists and could carry a `ktime_t recv_time` field.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-102",
      "title": "MEDIUM: tquic_napi struct layout",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** The `stats` field is updated every poll cycle and sits adjacent to `lock` and `rx_queue`. Writers on different CPUs (RX enqueue vs poll dequeue) will thrash the same cache lines.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.h",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.h:112-143"
        ],
        "symbols": [
          "CPUs",
          "fields"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.h:112-143"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Add `____cacheline_aligned_in_smp` between the RX-side fields (`rx_queue`, `rx_queue_len`, `lock`) and the poll-side fields (`stats`, `coalesce`) to separate them onto different cache lines.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-103",
      "title": "MEDIUM: Per-path stats updated from both RX and TX",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "Missing impact in source text.",
      "evidence": {
        "file_paths": [],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Split path stats into `____cacheline_aligned` RX and TX sections, or use per-CPU counters for path stats.",
      "tests_to_add": [],
      "notes": "Missing file path evidence in source text. Missing line-range evidence in source text. Missing Impact field in source text."
    },
    {
      "id": "F-104",
      "title": "MEDIUM: BBRv3 uses ktime_get_ns() for every bandwidth sample",
      "category": "perf",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Called on every ACK. Nanosecond precision is unnecessary for the windowed max filter which operates on RTT timescales.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/cong/bbrv3.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/cong/bbrv3.c:206"
        ],
        "symbols": [
          "bbrv3_update_bw",
          "ktime_get_ns"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/cong/bbrv3.c:206"
        ],
        "snippets": [
          "static void bbrv3_update_bw(struct bbrv3 *bbr, u64 acked, u64 rtt_us)\n{\n    u64 now = ktime_get_ns();"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Pass the timestamp from the caller rather than calling `ktime_get_ns()` again.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-105",
      "title": "LOW: Prague RTT scaling division on every ACK",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Division on every ACK in the Prague congestion control path. `rtt_target_us` is a constant for the connection lifetime.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/cong/prague.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/cong/prague.c:154"
        ],
        "symbols": [],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/cong/prague.c:154"
        ],
        "snippets": [
          "scaled = (value * rtt_us) / p->params.rtt_target_us;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Pre-compute a reciprocal multiplier for the RTT target to replace division with multiplication + shift.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-106",
      "title": "MEDIUM: Zerocopy entry refcount uses atomic_t (non-refcount_t)",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Using `atomic_t` for reference counting instead of `refcount_t` misses overflow/underflow protection. Not a performance issue but a correctness concern that could cause use-after-free.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:58-59"
        ],
        "symbols": [
          "atomic_t",
          "refcount_dec_and_test",
          "refcount_inc"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:58-59"
        ],
        "snippets": [
          "atomic_t        refcnt;         /* Reference count */"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Change to `refcount_t` and use `refcount_inc()` / `refcount_dec_and_test()`.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-107",
      "title": "MEDIUM: SmartNIC offload takes dev->lock for every key operation",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Key install/update holds the device lock while calling into the NIC driver's `add_key` callback, which may block or take significant time (MMIO, firmware command). This serializes all key operations across all connections using the same NIC.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/offload/smartnic.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/offload/smartnic.c:326-328"
        ],
        "symbols": [
          "add_key",
          "mutex",
          "time"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/offload/smartnic.c:326-328"
        ],
        "snippets": [
          "spin_lock(&dev->lock);\nret = dev->ops->add_key(dev, &key);\nspin_unlock(&dev->lock);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use a per-connection lock or a mutex (key operations are not in the data path and can sleep).",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-108",
      "title": "LOW: AF_XDP frame pool uses spinlock for every frame alloc/free",
      "category": "concurrency",
      "severity": "S3",
      "confidence": "low",
      "impact": "** Frame pool allocation and free both take `pool->lock`. At high packet rates, this is contended.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/af_xdp.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/af_xdp.c:222-238"
        ],
        "symbols": [
          "buffer"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/af_xdp.c:222-238"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use a lockless ring buffer (SPSC or MPSC depending on usage pattern) for the free list, similar to how io_uring and XDP use lockless rings.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-109",
      "title": "MEDIUM: Pacing work function drops and reacquires lock per packet",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Lock ping-pong per paced packet. Similar to the GRO flush issue.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1396-1421"
        ],
        "symbols": [
          "__skb_dequeue",
          "spin_lock_bh",
          "spin_unlock_bh",
          "tquic_output_packet"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1396-1421"
        ],
        "snippets": [
          "while (...) {\n    skb = __skb_dequeue(&pacing->queue);\n    spin_unlock_bh(&pacing->lock);\n    tquic_output_packet(NULL, pacing->path, skb);\n    spin_lock_bh(&pacing->lock);\n    ...\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Dequeue a batch of packets under a single lock hold, then send them all without the lock.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-110",
      "title": "LOW: pacing_calc_gap uses division",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": "** 64-bit division on every paced packet. Minor but avoidable.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1360"
        ],
        "symbols": [],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1360"
        ],
        "snippets": [
          "gap_ns = (u64)pkt_size * NSEC_PER_SEC / pacing->pacing_rate;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Pre-compute `ns_per_byte = NSEC_PER_SEC / pacing_rate` when the rate changes, then use multiplication: `gap_ns = pkt_size * ns_per_byte`.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-111",
      "title": "MEDIUM: GRO header parsing re-parses every held packet",
      "category": "api",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** For every incoming packet, all held packets are re-parsed to check for flow match. The header info could be cached in the NAPI_GRO_CB area.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c:235-253"
        ],
        "symbols": [
          "NAPI_GRO_CB",
          "list_for_each_entry",
          "skb_gro_offset",
          "tquic_gro_parse_header"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c:235-253"
        ],
        "snippets": [
          "list_for_each_entry(p, head, list) {\n    ...\n    ret = tquic_gro_parse_header(p->data + skb_gro_offset(p),\n                                 p->len, &held_hdr);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Cache the parsed `tquic_gro_header` in `NAPI_GRO_CB(skb)` on first parse, avoid re-parsing on subsequent comparisons.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-112",
      "title": "LOW: GRO hardcodes 8-byte CID for short headers",
      "category": "correctness",
      "severity": "S3",
      "confidence": "low",
      "impact": "** If connections use shorter CIDs, GRO comparison includes bytes beyond the CID, potentially preventing valid coalescing or coalescing different connections.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c:148"
        ],
        "symbols": [],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_offload.c:148"
        ],
        "snippets": [
          "hdr->dcid_len = TQUIC_DEFAULT_CID_LEN;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Store the negotiated CID length in a per-socket or per-connection field accessible from the GRO callback.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-113",
      "title": "MEDIUM: FEC encoder allocates per-symbol in GFP_ATOMIC",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Two GFP_ATOMIC allocations per source symbol. At block_size=8, that is 16 allocations per FEC block. Combined with the data copy, this adds significant overhead to the TX path when FEC is enabled.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/fec/fec_encoder.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/fec/fec_encoder.c:111-115"
        ],
        "symbols": [],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/fec/fec_encoder.c:111-115"
        ],
        "snippets": [
          "symbol = kzalloc(sizeof(*symbol), GFP_ATOMIC);\n...\nsymbol->data = kmalloc(length, GFP_ATOMIC);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "** Use slab caches for symbol structs and data buffers. Pre-allocate symbol arrays per block.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-114",
      "title": "LOW: XOR FEC encoding is efficient",
      "category": "concurrency",
      "severity": "S3",
      "confidence": "low",
      "impact": "Missing impact in source text.",
      "evidence": {
        "file_paths": [],
        "symbols": [
          "scheme"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing file path evidence in source text. Missing line-range evidence in source text. Missing Impact field in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-115",
      "title": "LOW: Minimal tracepoint overhead",
      "category": "memory",
      "severity": "S3",
      "confidence": "low",
      "impact": "Missing impact in source text.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/diag/trace.c"
        ],
        "symbols": [
          "FEC",
          "Recommendations",
          "ktime_get",
          "minimal",
          "skb_queue_splice_init",
          "tquic_find_path_by_addr",
          "tracepoints"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "Missing description/root cause in source text.",
      "fix_suggestion": "Missing fix suggestion in source text.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text. Missing Impact field in source text. Missing Recommendation/Fix field in source text."
    },
    {
      "id": "F-116",
      "title": "CRITICAL: NAPI busy_poll per-skb lock/unlock cycle",
      "category": "concurrency",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Under high packet rates with busy polling enabled, this creates severe lock contention. Each lock/unlock pair includes IRQ save/restore overhead, multiplied by every packet processed.\n- **Recommended fix:** Mirror the `tquic_napi_poll()` pattern -- use `skb_queue_splice_init()` to batch-dequeue into a local `sk_buff_head`, then process without holding the lock. Update `rx_queue_len` atomically by the batch count.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:460-465"
        ],
        "symbols": [
          "__skb_dequeue",
          "atomic_dec",
          "skb_queue_splice_init",
          "spin_lock_irqsave",
          "spin_unlock_irqrestore",
          "tquic_busy_poll",
          "tquic_napi_poll"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:460-465"
        ],
        "snippets": [
          "while (work_done < budget) {\n    spin_lock_irqsave(&tn->lock, flags);\n    skb = __skb_dequeue(&tn->rx_queue);\n    if (skb)\n        atomic_dec(&tn->rx_queue_len);\n    spin_unlock_irqrestore(&tn->lock, flags);\n    ...\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_busy_poll()` acquires and releases `tn->lock` once per packet in a tight loop. In contrast, `tquic_napi_poll()` at line 295 correctly uses `skb_queue_splice_init()` to batch-dequeue the entire queue under a single lock acquisition. The busy_poll path, which is specifically designed for latency-sensitive workloads, is performing far worse than the standard NAPI poll path due to this per-skb locking.\n- **Impact:** Under high packet rates with busy polling enabled, this creates severe lock contention. Each lock/unlock pair includes IRQ save/restore overhead, multiplied by every packet processed.\n- **Recommended fix:** Mirror the `tquic_napi_poll()` pattern -- use `skb_queue_splice_init()` to batch-dequeue into a local `sk_buff_head`, then process without holding the lock. Update `rx_queue_len` atomically by the batch count.",
      "fix_suggestion": "** Mirror the `tquic_napi_poll()` pattern -- use `skb_queue_splice_init()` to batch-dequeue into a local `sk_buff_head`, then process without holding the lock. Update `rx_queue_len` atomically by the batch count.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-117",
      "title": "HIGH: conn->lock released and reacquired during output flush stream iteration",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Two unnecessary atomic operations (cmpxchg or similar) per flush call. On systems with contended locks, this creates a window for preemption between the two acquisitions.\n- **Recommended fix:** Merge the two critical sections into one: check flow control credit and begin stream iteration under the same `conn->lock` hold.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:2058-2072"
        ],
        "symbols": [
          "operations",
          "rb_first",
          "rb_next",
          "spin_lock_bh",
          "spin_unlock_bh",
          "tquic_output_flush"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:2058-2072"
        ],
        "snippets": [
          "spin_lock_bh(&conn->lock);\nif (conn->data_sent >= conn->max_data_remote) {\n    spin_unlock_bh(&conn->lock);\n    ...\n}\nconn_credit = conn->max_data_remote - conn->data_sent;\nspin_unlock_bh(&conn->lock);\n\nspin_lock_bh(&conn->lock);\nfor (node = rb_first(&conn->streams); node && packets_sent < 16;\n     node = rb_next(node)) {"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_output_flush()` takes `conn->lock`, checks flow control, releases it, then immediately re-takes it for stream iteration. This is a redundant lock release/acquire cycle that costs two atomic operations for no benefit. The flow control check and stream iteration could be done under a single lock hold.\n- **Impact:** Two unnecessary atomic operations (cmpxchg or similar) per flush call. On systems with contended locks, this creates a window for preemption between the two acquisitions.\n- **Recommended fix:** Merge the two critical sections into one: check flow control credit and begin stream iteration under the same `conn->lock` hold.",
      "fix_suggestion": "** Merge the two critical sections into one: check flow control credit and begin stream iteration under the same `conn->lock` hold.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-118",
      "title": "HIGH: io_uring buffer ring spinlock per get/put operation",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Every io_uring receive/send operation takes two spinlock acquisitions (get buffer, then put buffer). Under high I/O rates, this serializes all buffer ring operations.\n- **Recommended fix:** If get/put are guaranteed to be called from different contexts (producer vs consumer), replace the spinlock with a lockless SPSC ring using `smp_store_release`/`smp_load_acquire` on head/tail. If multi-producer or multi-consumer, consider per-CPU rings or batch get/put APIs.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:804-838"
        ],
        "symbols": [
          "acquisitions",
          "contexts",
          "thread",
          "tquic_io_buf_ring_get",
          "tquic_io_buf_ring_put"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:804-838"
        ],
        "snippets": [
          "void *tquic_io_buf_ring_get(struct tquic_io_buf_ring *br, u16 *bid)\n{\n    spin_lock(&br->lock);\n    ...\n    spin_unlock(&br->lock);\n}\n\nvoid tquic_io_buf_ring_put(struct tquic_io_buf_ring *br, u16 bid)\n{\n    spin_lock(&br->lock);\n    ...\n    spin_unlock(&br->lock);\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The buffer ring is a classic single-producer/single-consumer ring buffer, but uses a spinlock for every get and put operation. Since io_uring submission and completion typically run on the same thread (or with well-defined ordering), a lockless SPSC ring with `smp_store_release`/`smp_load_acquire` would eliminate all lock overhead.\n- **Impact:** Every io_uring receive/send operation takes two spinlock acquisitions (get buffer, then put buffer). Under high I/O rates, this serializes all buffer ring operations.\n- **Recommended fix:** If get/put are guaranteed to be called from different contexts (producer vs consumer), replace the spinlock with a lockless SPSC ring using `smp_store_release`/`smp_load_acquire` on head/tail. If multi-producer or multi-consumer, consider per-CPU rings or batch get/put APIs.",
      "fix_suggestion": "** If get/put are guaranteed to be called from different contexts (producer vs consumer), replace the spinlock with a lockless SPSC ring using `smp_store_release`/`smp_load_acquire` on head/tail. If multi-producer or multi-consumer, consider per-CPU rings or batch get/put APIs.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-119",
      "title": "MEDIUM: Multiple lock acquisitions in RX hot path",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Moderate latency increase per packet, more significant under multipath where multiple paths may be processing concurrently.\n- **Recommended fix:** Consider a hierarchical locking strategy where `conn->lock` covers paths and streams for the common case, with finer-grained locks only for specific concurrent access patterns. Alternatively, use RCU for read-mostly structures like the stream rb-tree.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c"
        ],
        "symbols": [],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Processing a single incoming packet requires acquiring `conn->lock`, `conn->paths_lock`, `conn->streams_lock`, and potentially `conn->datagram.lock` at various points. Each lock acquisition/release pair adds cache line bouncing overhead.\n- **Impact:** Moderate latency increase per packet, more significant under multipath where multiple paths may be processing concurrently.\n- **Recommended fix:** Consider a hierarchical locking strategy where `conn->lock` covers paths and streams for the common case, with finer-grained locks only for specific concurrent access patterns. Alternatively, use RCU for read-mostly structures like the stream rb-tree.",
      "fix_suggestion": "** Consider a hierarchical locking strategy where `conn->lock` covers paths and streams for the common case, with finer-grained locks only for specific concurrent access patterns. Alternatively, use RCU for read-mostly structures like the stream rb-tree.",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-120",
      "title": "CRITICAL: Per-frame kzalloc + kmalloc in TX path",
      "category": "memory",
      "severity": "S0",
      "confidence": "high",
      "impact": "** Significant CPU overhead from slab allocator calls in the TX hot path. Under memory pressure, `GFP_ATOMIC` failures will cause partial sends and retransmissions.\n- **Recommended fix:** Create a dedicated `kmem_cache` for `struct tquic_pending_frame` (the RX path already uses `tquic_packet_cache` as a precedent). For frame data, consider embedding a small inline buffer (e.g., 128 bytes) in the frame struct for small frames, falling back to `kmalloc` only for larger chunks. Alternatively, use a per-connection slab or a free list of pre-allocated frames.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1806-1825"
        ],
        "symbols": [
          "allocations",
          "buffer",
          "tquic_xmit",
          "unlikely"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1806-1825"
        ],
        "snippets": [
          "/* Create pending frame */\nframe = kzalloc(sizeof(*frame), GFP_ATOMIC);\nif (unlikely(!frame)) {\n    ret = -ENOMEM;\n    break;\n}\n\nframe->type = TQUIC_FRAME_STREAM;\n...\nif (chunk > 0) {\n    frame->data = kmalloc(chunk, GFP_ATOMIC);\n    if (!frame->data) {\n        kfree(frame);\n        ret = -ENOMEM;\n        break;\n    }\n    memcpy(frame->data, data + offset, chunk);\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_xmit()` allocates two objects per STREAM frame: a `struct tquic_pending_frame` via `kzalloc` and a data buffer via `kmalloc`. For a typical 100KB transfer at 1200-byte chunks, this produces ~83 pairs of allocations (166 total `GFP_ATOMIC` allocations). `GFP_ATOMIC` allocations are more expensive than `GFP_KERNEL` and can fail under memory pressure.\n- **Impact:** Significant CPU overhead from slab allocator calls in the TX hot path. Under memory pressure, `GFP_ATOMIC` failures will cause partial sends and retransmissions.\n- **Recommended fix:** Create a dedicated `kmem_cache` for `struct tquic_pending_frame` (the RX path already uses `tquic_packet_cache` as a precedent). For frame data, consider embedding a small inline buffer (e.g., 128 bytes) in the frame struct for small frames, falling back to `kmalloc` only for larger chunks. Alternatively, use a per-connection slab or a free list of pre-allocated frames.",
      "fix_suggestion": "** Create a dedicated `kmem_cache` for `struct tquic_pending_frame` (the RX path already uses `tquic_packet_cache` as a precedent). For frame data, consider embedding a small inline buffer (e.g., 128 bytes) in the frame struct for small frames, falling back to `kmalloc` only for larger chunks. Alternatively, use a per-connection slab or a free list of pre-allocated frames.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-121",
      "title": "HIGH: Per-STREAM-frame skb allocation in RX path",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** One skb allocation and one memcpy per STREAM frame received, which can be thousands per second under load.\n- **Recommended fix:** Use `skb_clone()` or page fragment references to avoid copying. If the decryption buffer is page-backed, create frags pointing to the original pages with appropriate reference counting, eliminating both the allocation and the copy.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:944-948"
        ],
        "symbols": [
          "alloc_skb",
          "buffer",
          "skb_clone",
          "skb_put_data"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:944-948"
        ],
        "snippets": [
          "data_skb = alloc_skb(length, GFP_ATOMIC);\nif (!data_skb)\n    return -ENOMEM;\n\nskb_put_data(data_skb, ctx->data + ctx->offset, length);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Every incoming STREAM frame triggers a fresh `alloc_skb()` plus a full `memcpy` of the frame data. This is particularly wasteful because the data already exists in the decryption buffer (which itself came from a slab cache). The copy and allocation happen in softirq context with `GFP_ATOMIC`.\n- **Impact:** One skb allocation and one memcpy per STREAM frame received, which can be thousands per second under load.\n- **Recommended fix:** Use `skb_clone()` or page fragment references to avoid copying. If the decryption buffer is page-backed, create frags pointing to the original pages with appropriate reference counting, eliminating both the allocation and the copy.",
      "fix_suggestion": "** Use `skb_clone()` or page fragment references to avoid copying. If the decryption buffer is page-backed, create frags pointing to the original pages with appropriate reference counting, eliminating both the allocation and the copy.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-122",
      "title": "HIGH: kmalloc(path->mtu) per datagram send",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** One `GFP_ATOMIC` allocation and free per datagram sent.\n- **Recommended fix:** Use a per-connection pre-allocated scratch buffer (protected by `conn->lock` which is already held in the send path), or use a stack allocation since `path->mtu` is bounded and small (typically <= 1500 bytes; 1500 bytes on stack is acceptable in kernel context).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:2508"
        ],
        "symbols": [
          "buffer",
          "bytes",
          "small",
          "tquic_send_datagram"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:2508"
        ],
        "snippets": [
          "buf = kmalloc(path->mtu, GFP_ATOMIC);\nif (!buf)\n    return -ENOMEM;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_send_datagram()` allocates a temporary buffer of `path->mtu` bytes (typically 1200-1500 bytes) for every datagram send, uses it to build the frame, then frees it. This is a classic pattern that should use a per-CPU or per-connection pre-allocated buffer.\n- **Impact:** One `GFP_ATOMIC` allocation and free per datagram sent.\n- **Recommended fix:** Use a per-connection pre-allocated scratch buffer (protected by `conn->lock` which is already held in the send path), or use a stack allocation since `path->mtu` is bounded and small (typically <= 1500 bytes; 1500 bytes on stack is acceptable in kernel context).",
      "fix_suggestion": "** Use a per-connection pre-allocated scratch buffer (protected by `conn->lock` which is already held in the send path), or use a stack allocation since `path->mtu` is bounded and small (typically <= 1500 bytes; 1500 bytes on stack is acceptable in kernel context).",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-123",
      "title": "MEDIUM: Per-chunk skb allocation in zerocopy path",
      "category": "api",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Hundreds to thousands of skb allocations per large zerocopy send.\n- **Recommended fix:** Batch skb allocation using `alloc_skb_with_frags()` or `napi_alloc_skb()` for bulk allocation. Use the path MTU instead of hardcoded 1200. Consider using GSO to coalesce multiple chunks into fewer skbs.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:297"
        ],
        "symbols": [
          "alloc_skb",
          "alloc_skb_with_frags",
          "min_t",
          "napi_alloc_skb",
          "used"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:297"
        ],
        "snippets": [
          "while (copied < len) {\n    size_t chunk = min_t(size_t, len - copied, 1200);\n\n    new_skb = alloc_skb(0, GFP_KERNEL);\n    if (!new_skb) {\n        err = -ENOMEM;\n        goto out_err;\n    }"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The zerocopy send path allocates one skb per 1200-byte chunk. For a 1MB send, this is ~833 skb allocations. While `GFP_KERNEL` is used (allowing sleeping), the sheer volume of allocations is wasteful. The chunk size of 1200 is also suboptimal -- it should use the actual path MTU.\n- **Impact:** Hundreds to thousands of skb allocations per large zerocopy send.\n- **Recommended fix:** Batch skb allocation using `alloc_skb_with_frags()` or `napi_alloc_skb()` for bulk allocation. Use the path MTU instead of hardcoded 1200. Consider using GSO to coalesce multiple chunks into fewer skbs.",
      "fix_suggestion": "** Batch skb allocation using `alloc_skb_with_frags()` or `napi_alloc_skb()` for bulk allocation. Use the path MTU instead of hardcoded 1200. Consider using GSO to coalesce multiple chunks into fewer skbs.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-124",
      "title": "MEDIUM: kzalloc per io_uring async request",
      "category": "memory",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** One slab allocation per io_uring request, which under high-frequency I/O patterns can be thousands per second.\n- **Recommended fix:** Use `io_alloc_async_data()` (io_uring's internal API) or the req's inline async data storage if the struct fits. This avoids the general allocator and leverages io_uring's pre-allocated memory pools.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:139"
        ],
        "symbols": [
          "io_alloc_async_data"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:139"
        ],
        "snippets": [
          "data = kzalloc(sizeof(*data), GFP_KERNEL);\nif (!data)\n    return NULL;\n\nreq->async_data = data;\nreq->flags |= REQ_F_ASYNC_DATA;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Every io_uring TQUIC request allocates async data via `kzalloc`. io_uring has built-in mechanisms for async data that use the io_uring memory pool, avoiding the general-purpose allocator.\n- **Impact:** One slab allocation per io_uring request, which under high-frequency I/O patterns can be thousands per second.\n- **Recommended fix:** Use `io_alloc_async_data()` (io_uring's internal API) or the req's inline async data storage if the struct fits. This avoids the general allocator and leverages io_uring's pre-allocated memory pools.",
      "fix_suggestion": "** Use `io_alloc_async_data()` (io_uring's internal API) or the req's inline async data storage if the struct fits. This avoids the general allocator and leverages io_uring's pre-allocated memory pools.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-125",
      "title": "HIGH: struct tquic_napi mixes hot and cold fields",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Cache line bouncing between CPUs, particularly harmful in multi-queue NIC configurations where different CPUs poll different NAPI instances but may read each other's coalesce settings.\n- **Recommended fix:** Reorganize the struct with `____cacheline_aligned_in_smp` annotations:\n  1. Group read-mostly fields (config, coalesce, enabled flags) together\n  2. Group write-heavy fields (stats, state, queue_len) together with explicit cache line alignment\n  3. Place the lock on its own cache line",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.h"
        ],
        "symbols": [
          "fields"
        ],
        "line_ranges": [],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The `struct tquic_napi` packs frequently-written fields (stats counters, `rx_queue_len` atomic, `state`) alongside rarely-changed configuration fields (coalesce parameters, `busy_poll_enabled`) and the NAPI struct itself. When the poll path increments `stats.poll_cycles`, it dirties the cache line that also contains `coalesce` configuration, causing unnecessary cache invalidations on other CPUs reading the coalesce config.\n- **Impact:** Cache line bouncing between CPUs, particularly harmful in multi-queue NIC configurations where different CPUs poll different NAPI instances but may read each other's coalesce settings.\n- **Recommended fix:** Reorganize the struct with `____cacheline_aligned_in_smp` annotations:\n  1. Group read-mostly fields (config, coalesce, enabled flags) together\n  2. Group write-heavy fields (stats, state, queue_len) together with explicit cache line alignment\n  3. Place the lock on its own cache line",
      "fix_suggestion": "** Reorganize the struct with `____cacheline_aligned_in_smp` annotations:\n  1. Group read-mostly fields (config, coalesce, enabled flags) together\n  2. Group write-heavy fields (stats, state, queue_len) together with explicit cache line alignment\n  3. Place the lock on its own cache line",
      "tests_to_add": [],
      "notes": "Missing line-range evidence in source text."
    },
    {
      "id": "F-126",
      "title": "MEDIUM: Global atomic64 stats counters shared across CPUs",
      "category": "api",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Cache line bouncing on the global stats structure for every poll/busy_poll invocation across all CPUs.\n- **Recommended fix:** Remove the global atomic counters entirely. They are redundant with the per-CPU counters. Aggregate per-CPU stats on-demand when reading via procfs/sysfs (the `tquic_napi_aggregate_pcpu_stats()` function already exists for this purpose).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:308,477"
        ],
        "symbols": [
          "atomic64_add",
          "atomic64_inc",
          "sysfs",
          "tquic_napi_aggregate_pcpu_stats"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:308,477"
        ],
        "snippets": [
          "atomic64_inc(&tquic_napi_global_stats.total_polls);     // line 308\natomic64_add(work_done, &tquic_napi_global_stats.busy_poll_packets); // line 477"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Global `atomic64` counters are incremented on every NAPI poll cycle and every busy_poll cycle, in addition to per-CPU counters. The global atomics create a shared cache line that bounces between all CPUs doing NAPI processing.\n- **Impact:** Cache line bouncing on the global stats structure for every poll/busy_poll invocation across all CPUs.\n- **Recommended fix:** Remove the global atomic counters entirely. They are redundant with the per-CPU counters. Aggregate per-CPU stats on-demand when reading via procfs/sysfs (the `tquic_napi_aggregate_pcpu_stats()` function already exists for this purpose).",
      "fix_suggestion": "** Remove the global atomic counters entirely. They are redundant with the per-CPU counters. Aggregate per-CPU stats on-demand when reading via procfs/sysfs (the `tquic_napi_aggregate_pcpu_stats()` function already exists for this purpose).",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-127",
      "title": "HIGH: Double copy in RX STREAM frame processing",
      "category": "memory",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** For a 10 Gbps link with ~1200-byte frames, this is approximately 1M copies/sec, each touching both source and destination cache lines.\n- **Recommended fix:** Avoid the second copy. Options:\n  1. Decrypt directly into the skb's data area (requires pre-allocating the skb before decryption)\n  2. Use page fragments: decrypt into page-backed buffers, then add page frags to skb via `skb_fill_page_desc()` with `get_page()` reference\n  3. Use `skb_copy_bits()` with scatter-gather to reference the existing buffer",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:944-948"
        ],
        "symbols": [
          "alloc_skb",
          "area",
          "buffer",
          "get_page",
          "skb_copy_bits",
          "skb_fill_page_desc",
          "skb_put_data"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_input.c:944-948"
        ],
        "snippets": [
          "data_skb = alloc_skb(length, GFP_ATOMIC);\n...\nskb_put_data(data_skb, ctx->data + ctx->offset, length);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Data arriving from the network is first decrypted into a slab-allocated buffer (`ctx->data`), then copied again into a freshly allocated skb via `skb_put_data()`. This is a full copy of every byte of STREAM data received.\n- **Impact:** For a 10 Gbps link with ~1200-byte frames, this is approximately 1M copies/sec, each touching both source and destination cache lines.\n- **Recommended fix:** Avoid the second copy. Options:\n  1. Decrypt directly into the skb's data area (requires pre-allocating the skb before decryption)\n  2. Use page fragments: decrypt into page-backed buffers, then add page frags to skb via `skb_fill_page_desc()` with `get_page()` reference\n  3. Use `skb_copy_bits()` with scatter-gather to reference the existing buffer",
      "fix_suggestion": "** Avoid the second copy. Options:\n  1. Decrypt directly into the skb's data area (requires pre-allocating the skb before decryption)\n  2. Use page fragments: decrypt into page-backed buffers, then add page frags to skb via `skb_fill_page_desc()` with `get_page()` reference\n  3. Use `skb_copy_bits()` with scatter-gather to reference the existing buffer",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-128",
      "title": "MEDIUM: memcpy of TX frame data from userspace into frame, then into skb",
      "category": "correctness",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Every byte sent is copied twice through the TX path before reaching the NIC driver.\n- **Recommended fix:** Consider a zero-copy frame design where the pending frame holds a reference (page frag or skb frag) to the original data rather than copying it. For the non-zerocopy path, at minimum use `copy_from_iter()` directly into the final skb to eliminate one copy.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1825"
        ],
        "symbols": [
          "copy_from_iter",
          "reference",
          "tquic_xmit"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1825"
        ],
        "snippets": [
          "memcpy(frame->data, data + offset, chunk);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** In `tquic_xmit()`, data is copied from the source buffer into `frame->data` (a newly kmalloc'd buffer), then later when the packet is assembled, it gets copied again into the skb. This double-copy is inherent in the pending frame design.\n- **Impact:** Every byte sent is copied twice through the TX path before reaching the NIC driver.\n- **Recommended fix:** Consider a zero-copy frame design where the pending frame holds a reference (page frag or skb frag) to the original data rather than copying it. For the non-zerocopy path, at minimum use `copy_from_iter()` directly into the final skb to eliminate one copy.",
      "fix_suggestion": "** Consider a zero-copy frame design where the pending frame holds a reference (page frag or skb frag) to the original data rather than copying it. For the non-zerocopy path, at minimum use `copy_from_iter()` directly into the final skb to eliminate one copy.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-129",
      "title": "HIGH: Infinite retry loop on EMSGSIZE/EEXIST",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Potential kernel soft-lockup or hang in the sendmsg syscall path. This is exploitable by any local user with a TQUIC socket.\n- **Recommended fix:** Add a retry counter (e.g., max 3 retries) and return an error after exhausting retries. Alternatively, adjust the chunk size downward on EMSGSIZE before retrying.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:315-318"
        ],
        "symbols": [
          "TQUIC_SKB_ZEROCOPY_ITER_STREAM",
          "counter",
          "persistent"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:315-318"
        ],
        "snippets": [
          "if (err == -EMSGSIZE || err == -EEXIST) {\n    /* Try with a new skb */\n    continue;\n}"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** When `TQUIC_SKB_ZEROCOPY_ITER_STREAM()` returns `-EMSGSIZE` or `-EEXIST`, the loop frees the current skb and retries with a new one -- but `copied` is not advanced, so the same chunk is retried indefinitely. If the error condition is persistent (e.g., the chunk size always exceeds some limit), this becomes an infinite loop in kernel context.\n- **Impact:** Potential kernel soft-lockup or hang in the sendmsg syscall path. This is exploitable by any local user with a TQUIC socket.\n- **Recommended fix:** Add a retry counter (e.g., max 3 retries) and return an error after exhausting retries. Alternatively, adjust the chunk size downward on EMSGSIZE before retrying.",
      "fix_suggestion": "** Add a retry counter (e.g., max 3 retries) and return an error after exhausting retries. Alternatively, adjust the chunk size downward on EMSGSIZE before retrying.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-130",
      "title": "MEDIUM: Hardcoded 1200-byte chunk size ignores path MTU",
      "category": "api",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** ~17% more skb allocations than necessary on a typical 1452-byte MTU path.\n- **Recommended fix:** Use the connection's path MTU (minus overhead for QUIC headers) instead of the hardcoded 1200.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:293"
        ],
        "symbols": [
          "MTU",
          "min_t",
          "size"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_zerocopy.c:293"
        ],
        "snippets": [
          "size_t chunk = min_t(size_t, len - copied, 1200);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** The zerocopy path hardcodes 1200 bytes as the chunk size (QUIC minimum MTU), ignoring the actual path MTU which could be 1452 or higher. This creates more skbs and more overhead than necessary on paths with larger MTUs.\n- **Impact:** ~17% more skb allocations than necessary on a typical 1452-byte MTU path.\n- **Recommended fix:** Use the connection's path MTU (minus overhead for QUIC headers) instead of the hardcoded 1200.",
      "fix_suggestion": "** Use the connection's path MTU (minus overhead for QUIC headers) instead of the hardcoded 1200.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-131",
      "title": "HIGH: Redundant triple-counting of statistics",
      "category": "api",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Three memory writes (one of which is an atomic RMW on a shared cache line) for every NAPI poll invocation. The atomic global counter is particularly harmful as it serializes across all CPUs.\n- **Recommended fix:** Keep only per-CPU stats. Remove `tn->stats` (or make it derived) and remove global atomic counters. Aggregate on-demand via `tquic_napi_aggregate_pcpu_stats()` for reporting.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:306-308"
        ],
        "symbols": [
          "atomic64_inc",
          "this_cpu_inc",
          "tquic_napi_aggregate_pcpu_stats",
          "writes"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:306-308"
        ],
        "snippets": [
          "tn->stats.poll_cycles++;\nthis_cpu_inc(tquic_napi_pcpu_stats.total_polls);\natomic64_inc(&tquic_napi_global_stats.total_polls);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Every NAPI poll cycle increments three separate counters: the per-NAPI instance stats, per-CPU stats, and global atomic stats. This is triply redundant. The per-CPU stats can be aggregated to produce global stats, and the per-NAPI stats are a subset of per-CPU stats.\n- **Impact:** Three memory writes (one of which is an atomic RMW on a shared cache line) for every NAPI poll invocation. The atomic global counter is particularly harmful as it serializes across all CPUs.\n- **Recommended fix:** Keep only per-CPU stats. Remove `tn->stats` (or make it derived) and remove global atomic counters. Aggregate on-demand via `tquic_napi_aggregate_pcpu_stats()` for reporting.",
      "fix_suggestion": "** Keep only per-CPU stats. Remove `tn->stats` (or make it derived) and remove global atomic counters. Aggregate on-demand via `tquic_napi_aggregate_pcpu_stats()` for reporting.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-132",
      "title": "MEDIUM: atomic_inc/dec for rx_queue_len on every enqueue/dequeue",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** One atomic RMW per packet enqueued and dequeued.\n- **Recommended fix:** Remove `rx_queue_len` atomic. Use `skb_queue_len(&tn->rx_queue)` when the length is needed (it reads `qlen` from the queue head, which is maintained by `__skb_queue_tail`/`__skb_dequeue` already under the queue's lock).",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:464,689"
        ],
        "symbols": [
          "count",
          "needed",
          "skb_queue_len",
          "skb_queue_splice_init"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:464,689"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `rx_queue_len` is maintained as an `atomic_t` and incremented on every enqueue, decremented on every dequeue. However, in the NAPI poll path the queue is drained via `skb_queue_splice_init()` which already knows the count (via `skb_queue_len()`), making the separate atomic counter redundant.\n- **Impact:** One atomic RMW per packet enqueued and dequeued.\n- **Recommended fix:** Remove `rx_queue_len` atomic. Use `skb_queue_len(&tn->rx_queue)` when the length is needed (it reads `qlen` from the queue head, which is maintained by `__skb_queue_tail`/`__skb_dequeue` already under the queue's lock).",
      "fix_suggestion": "** Remove `rx_queue_len` atomic. Use `skb_queue_len(&tn->rx_queue)` when the length is needed (it reads `qlen` from the queue head, which is maintained by `__skb_queue_tail`/`__skb_dequeue` already under the queue's lock).",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-133",
      "title": "HIGH: Kernel address stored as u64 in buffer ring entries",
      "category": "security",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** Potential kernel address leak to userspace, defeating KASLR. This is a security concern in addition to a design concern.\n- **Recommended fix:** Use buffer IDs (indices) rather than raw kernel addresses. Store the base address separately in a kernel-only structure and compute the buffer address from `base + bid * size` at use time, never exposing the kernel address in shared structures.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:833"
        ],
        "symbols": [
          "IDs",
          "userspace"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:833"
        ],
        "snippets": [
          "br->br->bufs[tail].addr = (u64)(br->buf_base + bid * br->buf_size);"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** A kernel virtual address is cast to `u64` and stored in a buffer ring entry struct field named `addr`. If this struct is ever exposed to userspace (which is the normal io_uring buffer ring model), this leaks kernel ASLR. Even if the struct is kernel-only, this pattern is fragile and error-prone.\n- **Impact:** Potential kernel address leak to userspace, defeating KASLR. This is a security concern in addition to a design concern.\n- **Recommended fix:** Use buffer IDs (indices) rather than raw kernel addresses. Store the base address separately in a kernel-only structure and compute the buffer address from `base + bid * size` at use time, never exposing the kernel address in shared structures.",
      "fix_suggestion": "** Use buffer IDs (indices) rather than raw kernel addresses. Store the base address separately in a kernel-only structure and compute the buffer address from `base + bid * size` at use time, never exposing the kernel address in shared structures.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-134",
      "title": "MEDIUM: Missing batch API for buffer ring operations",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Lock overhead scales linearly with batch size.\n- **Recommended fix:** Add batch APIs: `tquic_io_buf_ring_get_batch(br, bufs, bids, count)` and `tquic_io_buf_ring_put_batch(br, bids, count)` that acquire the lock once for multiple operations.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:799-838"
        ],
        "symbols": [
          "tquic_io_buf_ring_get",
          "tquic_io_buf_ring_get_batch",
          "tquic_io_buf_ring_put",
          "tquic_io_buf_ring_put_batch"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/io_uring.c:799-838"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** `tquic_io_buf_ring_get()` and `tquic_io_buf_ring_put()` acquire and release the spinlock for every single buffer operation. When processing a batch of I/O completions, this means N lock/unlock pairs instead of one.\n- **Impact:** Lock overhead scales linearly with batch size.\n- **Recommended fix:** Add batch APIs: `tquic_io_buf_ring_get_batch(br, bufs, bids, count)` and `tquic_io_buf_ring_put_batch(br, bids, count)` that acquire the lock once for multiple operations.",
      "fix_suggestion": "** Add batch APIs: `tquic_io_buf_ring_get_batch(br, bufs, bids, count)` and `tquic_io_buf_ring_put_batch(br, bids, count)` that acquire the lock once for multiple operations.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-135",
      "title": "HIGH: atomic64_inc_return for packet number on every TX",
      "category": "concurrency",
      "severity": "S1",
      "confidence": "medium",
      "impact": "** One atomic RMW (with implicit memory barrier) per TX packet.\n- **Recommended fix:** If the TX path is always serialized by `conn->lock`, replace with a plain `u64` increment. If not always locked, document which paths require the atomic and consider whether a per-path packet number counter with a regular lock would be more appropriate.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1785"
        ],
        "symbols": [
          "RMW",
          "atomic64_inc_return"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/tquic_output.c:1785"
        ],
        "snippets": [
          "pkt_num = atomic64_inc_return(&conn->pkt_num_tx) - 1;"
        ],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Every packet sent uses `atomic64_inc_return()` to generate a packet number. This is a full memory barrier atomic RMW operation. However, the TX path is typically serialized by `conn->lock` (or should be, since packet numbers must be monotonic and gap-free within a packet number space). If `conn->lock` is already held, the atomic is unnecessary overhead.\n- **Impact:** One atomic RMW (with implicit memory barrier) per TX packet.\n- **Recommended fix:** If the TX path is always serialized by `conn->lock`, replace with a plain `u64` increment. If not always locked, document which paths require the atomic and consider whether a per-path packet number counter with a regular lock would be more appropriate.",
      "fix_suggestion": "** If the TX path is always serialized by `conn->lock`, replace with a plain `u64` increment. If not always locked, document which paths require the atomic and consider whether a per-path packet number counter with a regular lock would be more appropriate.",
      "tests_to_add": [],
      "notes": ""
    },
    {
      "id": "F-136",
      "title": "MEDIUM: Multiple atomic operations in NAPI enqueue path",
      "category": "concurrency",
      "severity": "S2",
      "confidence": "medium",
      "impact": "** Cumulative overhead on high-packet-rate workloads.\n- **Recommended fix:** Remove the redundant `rx_queue_len` atomic (as noted above). This reduces to the minimum of one lock acquisition per enqueue.",
      "evidence": {
        "file_paths": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c",
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:689"
        ],
        "symbols": [
          "Line",
          "atomic",
          "atomic_inc"
        ],
        "line_ranges": [
          "/Users/justinadams/Downloads/tquic-kernel/net/tquic/napi.c:689"
        ],
        "snippets": [],
        "logs_or_errors": []
      },
      "repro": {
        "steps": [],
        "expected": "",
        "actual": ""
      },
      "root_cause_hypothesis": "** Enqueuing a single packet into the NAPI RX queue involves: `spin_lock_irqsave` (atomic CAS), `__skb_queue_tail` (increments qlen), `atomic_inc(&tn->rx_queue_len)`, `spin_unlock_irqrestore`. That is at least 3 atomic operations per packet in the hot enqueue path.\n- **Impact:** Cumulative overhead on high-packet-rate workloads.\n- **Recommended fix:** Remove the redundant `rx_queue_len` atomic (as noted above). This reduces to the minimum of one lock acquisition per enqueue.",
      "fix_suggestion": "** Remove the redundant `rx_queue_len` atomic (as noted above). This reduces to the minimum of one lock acquisition per enqueue.",
      "tests_to_add": [],
      "notes": ""
    }
  ],
  "meta": {
    "extraction_quality": "good",
    "why_failed": "",
    "notes": [
      "Normalized from audit markdown files: AUDIT_HIGH_STAKES_CODE_REVIEW.md, AUDIT_CORE_CORRECTNESS.md, AUDIT_CRYPTO_SECURITY.md, AUDIT_SECURITY_NETLINK_LB.md, AUDIT_MULTIPATH_BONDING.md, AUDIT_HTTP3_MISC.md, AUDIT_PERFORMANCE.md, AUDIT_PERF_HOTPATH.md",
      "This JSON is derived from the audit docs in-repo (not a separately pasted RAW_FINDINGS blob)."
    ]
  }
}